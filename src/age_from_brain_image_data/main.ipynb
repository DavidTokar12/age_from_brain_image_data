{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "e8745ff7",
      "metadata": {
        "id": "e8745ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c9c5f3-ee43-4de7-da84-6de17c5a872b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.gaussian_process.kernels as kernels\n",
        "\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import StackingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.svm import NuSVR\n",
        "from joblib import Memory\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import clone\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor"
      ],
      "metadata": {
        "id": "eQvw6nGFv2Ye"
      },
      "id": "eQvw6nGFv2Ye",
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyoNxh7100Bo",
        "outputId": "b719e5fa-8296-451e-e7b3-3b666ec3bd55"
      },
      "id": "XyoNxh7100Bo",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "78ab9db9",
      "metadata": {
        "id": "78ab9db9"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = Path(\"/content/gdrive/MyDrive/data\")\n",
        "x_train = pd.read_csv(BASE_PATH / 'X_train.csv', skiprows=1, header=None).values[:, 1:]\n",
        "x_test = pd.read_csv(BASE_PATH / 'X_test.csv', skiprows=1, header=None).values[:, 1:]\n",
        "y_train = pd.read_csv(BASE_PATH / 'y_train.csv', skiprows=1, header=None).values[:, 1:].flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "a7687ba6",
      "metadata": {
        "id": "a7687ba6"
      },
      "outputs": [],
      "source": [
        "def remove_outliers(x_train, y_train, x_test, contamination=0.047, variance_threshold=1e-7, random_state=42):\n",
        "    \"\"\"\n",
        "    Remove low-variance features and outliers from training data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    x_train : DataFrame or ndarray\n",
        "    y_train : Series or ndarray\n",
        "    x_test : DataFrame or ndarray\n",
        "    contamination : float, proportion of outliers\n",
        "    variance_threshold : float, threshold for variance filtering\n",
        "    random_state : int\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    x_train_clean, y_train_clean, x_test_clean\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to DataFrame if needed\n",
        "    x_train_df = pd.DataFrame(x_train).copy()\n",
        "    x_test_df = pd.DataFrame(x_test).copy() if x_test is not None else None\n",
        "    y_train_series = pd.Series(np.asarray(y_train).flatten(), index=x_train_df.index)\n",
        "\n",
        "    # ===== Step 1: Remove zero/low-variance features =====\n",
        "    var_selector = VarianceThreshold(threshold=variance_threshold)\n",
        "\n",
        "    # Fit on training data (using median-imputed values to handle NaN)\n",
        "    med = x_train_df.median(axis=0)\n",
        "    x_train_for_var = x_train_df.fillna(med)\n",
        "\n",
        "    var_selector.fit(x_train_for_var)\n",
        "\n",
        "    variance_mask = var_selector.get_support()\n",
        "    n_removed = (~variance_mask).sum()\n",
        "    print(f\"[VarianceThreshold] Removed {n_removed} low-variance features (threshold={variance_threshold})\")\n",
        "\n",
        "    x_train_df = x_train_df.iloc[:, variance_mask]\n",
        "    if x_test_df is not None:\n",
        "        x_test_df = x_test_df.iloc[:, variance_mask]\n",
        "\n",
        "    med = x_train_df.median(axis=0)\n",
        "    xtr_imp = x_train_df.fillna(med)\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    xtr_std = scaler.fit_transform(xtr_imp)\n",
        "\n",
        "    pca = PCA(n_components=2, random_state=random_state)\n",
        "    x_proj = pca.fit_transform(xtr_std)\n",
        "\n",
        "    iso = IsolationForest(contamination=contamination, random_state=random_state)\n",
        "    mask = iso.fit_predict(x_proj) == 1\n",
        "\n",
        "    outlier_pos = np.where(~mask)[0]\n",
        "    n_outliers = outlier_pos.size\n",
        "    print(f\"[OutlierRemoval] Removed {n_outliers} outliers ({n_outliers/len(mask)*100:.2f}%)\")\n",
        "    print(f\"[Outlier positions] {outlier_pos.tolist()}\")\n",
        "\n",
        "    x_train_clean = x_train_df[mask]\n",
        "    y_train_clean = y_train_series[mask]\n",
        "\n",
        "    if isinstance(x_train, np.ndarray):\n",
        "        return x_train_clean.values, y_train_clean.values, x_test_df.values if x_test_df is not None else None\n",
        "    else:\n",
        "        return x_train_clean, y_train_clean, x_test_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledKNNImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    KNN Imputer that internally scales data for distance calculation,\n",
        "    then returns imputed data in the ORIGINAL scale.\n",
        "\n",
        "    This avoids having scaling artifacts in your pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neighbors=7, weights='distance'):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.weights = weights\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_df = pd.DataFrame(X)\n",
        "\n",
        "        self.median_ = X_df.median(axis=0)\n",
        "        X_filled = X_df.fillna(self.median_)\n",
        "\n",
        "        self.scaler_ = StandardScaler()\n",
        "        self.scaler_.fit(X_filled)\n",
        "\n",
        "        X_scaled = self.scaler_.transform(X_filled)\n",
        "        self.imputer_ = KNNImputer(n_neighbors=self.n_neighbors, weights=self.weights)\n",
        "        self.imputer_.fit(X_scaled)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_df = pd.DataFrame(X)\n",
        "\n",
        "        X_filled = X_df.fillna(self.median_)\n",
        "\n",
        "        X_scaled = self.scaler_.transform(X_filled)\n",
        "\n",
        "        X_imputed_scaled = self.imputer_.transform(X_scaled)\n",
        "\n",
        "        X_imputed_original = self.scaler_.inverse_transform(X_imputed_scaled)\n",
        "\n",
        "        return X_imputed_original"
      ],
      "metadata": {
        "id": "jXveDNpyRypS"
      },
      "id": "jXveDNpyRypS",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "381fe6a3",
      "metadata": {
        "id": "381fe6a3"
      },
      "outputs": [],
      "source": [
        "class SpearmanRFSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Combined feature selector:\n",
        "    1. Scale data (using provided scaler)\n",
        "    2. Select top_k features by Spearman correlation\n",
        "    3. Select top_k features by RandomForest importance\n",
        "\n",
        "    Works on already-imputed data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        scaler=None,\n",
        "        feature_tops=(200, 200),  # (spearman_top_k, rf_top_k) as tuple\n",
        "        rf_n_estimators=1000,\n",
        "        rf_max_depth=None,\n",
        "        rf_min_samples_leaf=1,\n",
        "        rf_max_features='sqrt',\n",
        "        random_state=42\n",
        "    ):\n",
        "        self.scaler = scaler\n",
        "        self.feature_tops = feature_tops\n",
        "        self.rf_n_estimators = rf_n_estimators\n",
        "        self.rf_max_depth = rf_max_depth\n",
        "        self.rf_min_samples_leaf = rf_min_samples_leaf\n",
        "        self.rf_max_features = rf_max_features\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if y is None:\n",
        "            raise ValueError(\"SpearmanRFSelector requires y\")\n",
        "\n",
        "        spearman_top_k, rf_top_k = self.feature_tops\n",
        "\n",
        "        X_df = pd.DataFrame(X).copy() if not isinstance(X, pd.DataFrame) else X.copy()\n",
        "        y_series = pd.Series(np.asarray(y).flatten(), index=X_df.index)\n",
        "\n",
        "        self.n_features_in_ = X_df.shape[1]\n",
        "\n",
        "        if self.scaler is not None:\n",
        "            self.scaler_ = clone(self.scaler)\n",
        "            X_scaled = self.scaler_.fit_transform(X_df)\n",
        "            X_scaled_df = pd.DataFrame(X_scaled, columns=X_df.columns, index=X_df.index)\n",
        "        else:\n",
        "            X_scaled_df = X_df\n",
        "\n",
        "        spearman_corr = X_scaled_df.corrwith(y_series, method='spearman').abs()\n",
        "\n",
        "        n_keep_spearman = min(spearman_top_k, len(spearman_corr))\n",
        "        spearman_features = spearman_corr.nlargest(n_keep_spearman).index\n",
        "\n",
        "        print(f\"[SpearmanRFSelector] Spearman: Selected {len(spearman_features)} features (from {self.n_features_in_})\")\n",
        "\n",
        "        X_spearman = X_scaled_df.loc[:, spearman_features]\n",
        "\n",
        "        self.rf_ = RandomForestRegressor(\n",
        "            n_estimators=self.rf_n_estimators,\n",
        "            max_depth=self.rf_max_depth,\n",
        "            min_samples_leaf=self.rf_min_samples_leaf,\n",
        "            max_features=self.rf_max_features,\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        self.rf_.fit(X_spearman, y_series)\n",
        "\n",
        "        importances = pd.Series(self.rf_.feature_importances_, index=X_spearman.columns)\n",
        "        importances = importances.sort_values(ascending=False)\n",
        "\n",
        "        n_keep_rf = min(rf_top_k, len(importances)) if rf_top_k else len(importances)\n",
        "        rf_features = importances.head(n_keep_rf).index\n",
        "\n",
        "        print(f\"[SpearmanRFSelector] RF: Selected {len(rf_features)} features (from {len(spearman_features)})\")\n",
        "\n",
        "        self.selected_features_ = [X_df.columns.get_loc(col) for col in rf_features]\n",
        "        self.selected_feature_names_ = rf_features.tolist()\n",
        "\n",
        "        print(f\"[SpearmanRFSelector] FINAL: {len(self.selected_features_)} features\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
        "        # Return selected features\n",
        "        return X_df.iloc[:, self.selected_features_].values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_imputation_pipeline(random_state=42):\n",
        "    pipeline = Pipeline(\n",
        "        [\n",
        "              (\"knn_imputer\", ScaledKNNImputer()),\n",
        "            # (\"iterative_imputer\", IterativeImputer()),\n",
        "        ]\n",
        "    )\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "ivhM2nOC4W7A"
      },
      "id": "ivhM2nOC4W7A",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "668ce9c7",
      "metadata": {
        "id": "668ce9c7"
      },
      "outputs": [],
      "source": [
        "def build_feature_selection_pipeline(random_state=42):\n",
        "\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            random_state=random_state\n",
        "        ))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "98862227",
      "metadata": {
        "id": "98862227"
      },
      "outputs": [],
      "source": [
        "def build_svr_branch(random_state=42):\n",
        "    \"\"\"SVR branch with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", NuSVR())\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_hgb_branch(random_state=42):\n",
        "    \"\"\"HistGradientBoosting with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", HistGradientBoostingRegressor(random_state=random_state))\n",
        "    ])\n",
        "\n",
        "def build_xgb_branch(random_state=42):\n",
        "    \"\"\"XGBoost\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", RobustScaler()),\n",
        "        (\"model\", XGBRegressor(\n",
        "            random_state=random_state,\n",
        "            n_jobs=-1,\n",
        "            verbosity=0  # Suppress output\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_cat_branch(random_state=42):\n",
        "    \"\"\"CatBoost\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", RobustScaler()),\n",
        "        (\"model\", CatBoostRegressor(\n",
        "            random_state=random_state,\n",
        "            verbose=False,\n",
        "            thread_count=-1\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_etr_branch(random_state=42):\n",
        "    \"\"\"ExtraTrees with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", ExtraTreesRegressor(random_state=random_state, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "def build_gpr_branch(random_state=42):\n",
        "    \"\"\"GaussianProcess with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", RobustScaler()),\n",
        "        (\"model\", GaussianProcessRegressor(random_state=random_state))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "f7484d3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7484d3a",
        "outputId": "ba9cc5da-d32f-4a4c-c085-6f934c726092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VarianceThreshold] Removed 4 low-variance features (threshold=1e-07)\n",
            "[OutlierRemoval] Removed 57 outliers (4.70%)\n",
            "[Outlier positions] [3, 10, 27, 60, 114, 156, 167, 203, 207, 213, 232, 329, 346, 380, 382, 423, 441, 442, 446, 459, 537, 575, 579, 611, 627, 674, 676, 681, 690, 694, 697, 733, 740, 763, 769, 805, 899, 906, 915, 933, 935, 951, 983, 1015, 1032, 1070, 1071, 1087, 1090, 1097, 1103, 1127, 1136, 1137, 1144, 1152, 1196]\n"
          ]
        }
      ],
      "source": [
        "x_train_clean, y_train_clean, x_test_clean = remove_outliers(\n",
        "    x_train, y_train, x_test, contamination=0.047, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "svr_branch = build_svr_branch(random_state=42)\n",
        "\n",
        "svr_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"svr\", svr_branch),\n",
        "])\n",
        "\n",
        "svr_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "    \"svr__feature_selector__feature_tops\": [(200, 175)],\n",
        "    \"svr__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"svr__feature_selector__rf_max_depth\": [None],\n",
        "    \"svr__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"svr__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "    \"svr__model__nu\": [0.5],\n",
        "    \"svr__model__kernel\": [\"rbf\"],\n",
        "    \"svr__model__C\": [60],\n",
        "    \"svr__model__gamma\": [\"auto\"],\n",
        "}\n",
        "\n",
        "svr_grid = GridSearchCV(\n",
        "    estimator=svr_full_pipeline,\n",
        "    param_grid=svr_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing SVR Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "svr_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest SVR R²: {svr_grid.best_score_:.4f}\")\n",
        "print(f\"Best SVR Params: {svr_grid.best_params_}\")\n",
        "\n",
        "#  {\n",
        "#     \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "#     \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "#     \"svr__feature_selector__feature_tops\": [\n",
        "#         (50, 45),\n",
        "#         (75, 65),\n",
        "#         (100, 90),\n",
        "#         (125, 110),\n",
        "#         (150, 130),\n",
        "#         (175, 150),\n",
        "#         (200, 170),\n",
        "#         (225, 190),\n",
        "#         (250, 210),\n",
        "#         (275, 230),\n",
        "#         (300, 250),\n",
        "#         (350, 290),\n",
        "#         (400, 330),\n",
        "#         (450, 370),\n",
        "#         (500, 410),\n",
        "#     ],\n",
        "#     \"svr__feature_selector__rf_n_estimators\": [1000],\n",
        "#     \"svr__feature_selector__rf_max_depth\": [None],\n",
        "#     \"svr__feature_selector__rf_min_samples_leaf\": [1],\n",
        "#     \"svr__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "#     \"svr__model__nu\": [0.5],\n",
        "#     \"svr__model__kernel\": [\"rbf\"],\n",
        "#     \"svr__model__C\": [50, 55, 60],\n",
        "#     \"svr__model__gamma\": [\"auto\"],\n",
        "# }\n",
        "# Best SVR R²: 0.6731\n",
        "# Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 170), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n",
        "\n",
        "\n",
        "# \"svr__feature_selector__feature_tops\": [\n",
        "#       (170, 145),\n",
        "#       (175, 150),\n",
        "#       (180, 155),\n",
        "#       (185, 160),\n",
        "#       (190, 165),\n",
        "#       (195, 168),\n",
        "#       (198, 168),\n",
        "#       (200, 170),\n",
        "#       (202, 172),\n",
        "#       (205, 175),\n",
        "#       (210, 180),\n",
        "#       (215, 185),\n",
        "#       (220, 188),\n",
        "#       (225, 190),\n",
        "#       (230, 195),\n",
        "# ]\n",
        "# Best SVR R²: 0.6731\n",
        "# Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 170), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n",
        "\n",
        "# \"svr__feature_selector__feature_tops\": [\n",
        "#       (200, 161),\n",
        "#       (200, 162),\n",
        "#       (200, 163),\n",
        "#       (200, 164),\n",
        "#       (200, 165),\n",
        "#       (200, 166),\n",
        "#       (200, 167),\n",
        "#       (200, 168),\n",
        "#       (200, 169),\n",
        "#       (200, 170),\n",
        "#       (200, 171),\n",
        "#       (200, 172),\n",
        "#       (200, 173),\n",
        "#       (200, 174),\n",
        "#       (200, 175),\n",
        "#       (200, 176),\n",
        "#       (200, 177),\n",
        "#       (200, 178),\n",
        "#       (200, 179),\n",
        "#     ],\n",
        "# Best SVR R²: 0.6756\n",
        "# Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 175), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n",
        "\n",
        "# \"svr__model__nu\": [0.4, 0.5, 0.6],\n",
        "# \"svr__model__C\": [60, 65, 70],\n",
        "# Best SVR R²: 0.6756\n",
        "# Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 175), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk6xBz_mIoJz",
        "outputId": "3628c747-dac6-4bfa-e2d2-ccea7844f8a5"
      },
      "id": "hk6xBz_mIoJz",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing SVR Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n",
            "\n",
            "Best SVR R²: 0.6756\n",
            "Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 175), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "hgb_branch = build_hgb_branch(random_state=42)\n",
        "\n",
        "hgb_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"hgb\", hgb_branch),\n",
        "])\n",
        "\n",
        "hgb_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "\n",
        "    \"hgb__feature_selector__feature_tops\":[(200, 167)],\n",
        "    \"hgb__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"hgb__feature_selector__rf_max_depth\": [None],\n",
        "    \"hgb__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"hgb__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "\n",
        "    \"hgb__model__learning_rate\": [0.1],\n",
        "    \"hgb__model__max_iter\": [200],\n",
        "    \"hgb__model__max_depth\": [None],\n",
        "    \"hgb__model__min_samples_leaf\": [20],\n",
        "}\n",
        "\n",
        "hgb_grid = GridSearchCV(\n",
        "    estimator=hgb_full_pipeline,\n",
        "    param_grid=hgb_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing HGB Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "hgb_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest HGB R²: {hgb_grid.best_score_:.4f}\")\n",
        "print(f\"Best HGB Params: {hgb_grid.best_params_}\")\n",
        "\n",
        "\n",
        "# [\n",
        "#  (50, 45),\n",
        "#  (125, 110),\n",
        "#  (200, 170),\n",
        "#  (275, 230),\n",
        "#  (350, 290),\n",
        "#  (425, 350),\n",
        "#  (500, 410),\n",
        "# ],\n",
        "# Best HGB R²: 0.6367\n",
        "# Best HGB Params: {'hgb__feature_selector__feature_tops': (200, 170), 'hgb__feature_selector__rf_max_depth': None, 'hgb__feature_selector__rf_max_features': 'sqrt', 'hgb__feature_selector__rf_min_samples_leaf': 1, 'hgb__feature_selector__rf_n_estimators': 1000, 'hgb__model__learning_rate': 0.1, 'hgb__model__max_depth': None, 'hgb__model__max_iter': 100, 'hgb__model__min_samples_leaf': 20, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}\n",
        "\n",
        "# [\n",
        "#       (200, 161),\n",
        "#       (200, 162),\n",
        "#       (200, 163),\n",
        "#       (200, 164),\n",
        "#       (200, 165),\n",
        "#       (200, 166),\n",
        "#       (200, 167),\n",
        "#       (200, 168),\n",
        "#       (200, 169),\n",
        "#       (200, 170),\n",
        "#       (200, 171),\n",
        "#       (200, 172),\n",
        "#       (200, 173),\n",
        "#       (200, 174),\n",
        "#       (200, 175),\n",
        "#       (200, 176),\n",
        "#       (200, 177),\n",
        "#       (200, 178),\n",
        "#       (200, 179),\n",
        "# ]\n",
        "# Best HGB R²: 0.6424\n",
        "# Best HGB Params: {'hgb__feature_selector__feature_tops': (200, 167), 'hgb__feature_selector__rf_max_depth': None, 'hgb__feature_selector__rf_max_features': 'sqrt', 'hgb__feature_selector__rf_min_samples_leaf': 1, 'hgb__feature_selector__rf_n_estimators': 1000, 'hgb__model__learning_rate': 0.1, 'hgb__model__max_depth': None, 'hgb__model__max_iter': 100, 'hgb__model__min_samples_leaf': 20, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}\n",
        "\n",
        "\n",
        "# \"hgb__model__learning_rate\": [0.05, 0.1, 0.15],\n",
        "# \"hgb__model__max_iter\": [100, 150, 200],\n",
        "# Best HGB R²: 0.6449\n",
        "# Best HGB Params: {'hgb__feature_selector__feature_tops': (200, 167), 'hgb__feature_selector__rf_max_depth': None, 'hgb__feature_selector__rf_max_features': 'sqrt', 'hgb__feature_selector__rf_min_samples_leaf': 1, 'hgb__feature_selector__rf_n_estimators': 1000, 'hgb__model__learning_rate': 0.1, 'hgb__model__max_depth': None, 'hgb__model__max_iter': 200, 'hgb__model__min_samples_leaf': 20, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgdzIIBEqMzz",
        "outputId": "2034043d-998c-466d-d4ec-da41ad657b47"
      },
      "id": "RgdzIIBEqMzz",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing HGB Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 167 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 167 features\n",
            "\n",
            "Best HGB R²: 0.6449\n",
            "Best HGB Params: {'hgb__feature_selector__feature_tops': (200, 167), 'hgb__feature_selector__rf_max_depth': None, 'hgb__feature_selector__rf_max_features': 'sqrt', 'hgb__feature_selector__rf_min_samples_leaf': 1, 'hgb__feature_selector__rf_n_estimators': 1000, 'hgb__model__learning_rate': 0.1, 'hgb__model__max_depth': None, 'hgb__model__max_iter': 200, 'hgb__model__min_samples_leaf': 20, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "xgb_branch = build_xgb_branch(random_state=42)\n",
        "\n",
        "xgb_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"xgb\", xgb_branch),\n",
        "])\n",
        "\n",
        "xgb_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "    \"xgb__feature_selector__feature_tops\": [(200, 175)],\n",
        "    \"xgb__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"xgb__feature_selector__rf_max_depth\": [None],\n",
        "    \"xgb__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"xgb__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "    \"xgb__model__n_estimators\": [1000],\n",
        "    \"xgb__model__learning_rate\": [0.1],\n",
        "    \"xgb__model__max_depth\": [3],\n",
        "    \"xgb__model__min_child_weight\": [1.2],\n",
        "    \"xgb__model__subsample\": [1.0],\n",
        "    \"xgb__model__colsample_bytree\": [1.0],\n",
        "    \"xgb__model__gamma\": [0],\n",
        "}\n",
        "\n",
        "xgb_grid = GridSearchCV(\n",
        "    estimator=xgb_full_pipeline,\n",
        "    param_grid=xgb_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing XGBoost Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "xgb_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest XGBoost R²: {xgb_grid.best_score_:.4f}\")\n",
        "print(f\"Best XGBoost Params: {xgb_grid.best_params_}\")\n",
        "\n",
        "# [\n",
        "#         (50, 45),\n",
        "#         (75, 65),\n",
        "#         (100, 90),\n",
        "#         (125, 110),\n",
        "#         (150, 130),\n",
        "#         (175, 150),\n",
        "#         (200, 170),\n",
        "#         (225, 190),\n",
        "#         (250, 210),\n",
        "#         (275, 230),\n",
        "#         (300, 250),\n",
        "#         (350, 290),\n",
        "#         (400, 330),\n",
        "#         (450, 370),\n",
        "#         (500, 410),\n",
        "# ]\n",
        "# Best XGBoost R²: 0.6292\n",
        "# Best XGBoost Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'xgb__feature_selector__feature_tops': (200, 170), 'xgb__feature_selector__rf_max_depth': None, 'xgb__feature_selector__rf_max_features': 'sqrt', 'xgb__feature_selector__rf_min_samples_leaf': 1, 'xgb__feature_selector__rf_n_estimators': 1000, 'xgb__model__colsample_bytree': 0.8, 'xgb__model__gamma': 0, 'xgb__model__learning_rate': 0.1, 'xgb__model__max_depth': 6, 'xgb__model__min_child_weight': 1, 'xgb__model__n_estimators': 500, 'xgb__model__subsample': 0.8}\n",
        "\n",
        "#Best XGBoost R²: 0.6339\n",
        "# (200, 175)\n",
        "\n",
        "# Best XGBoost R²: 0.6434\n",
        "# \"xgb__model__n_estimators\": [500, 1000],\n",
        "# \"xgb__model__learning_rate\": [0.1, 0.3],\n",
        "# \"xgb__model__max_depth\": [3, 6, 9],\n",
        "# \"xgb__model__min_child_weight\": [1, 1.2],\n",
        "# \"xgb__model__subsample\": [0.8, 1.0],\n",
        "# \"xgb__model__colsample_bytree\": [0.8, 1.0],\n",
        "# \"xgb__model__gamma\": [0, 0.2],\n",
        "# {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'xgb__feature_selector__feature_tops': (200, 175), 'xgb__feature_selector__rf_max_depth': None, 'xgb__feature_selector__rf_max_features': 'sqrt', 'xgb__feature_selector__rf_min_samples_leaf': 1, 'xgb__feature_selector__rf_n_estimators': 1000, 'xgb__model__colsample_bytree': 1.0, 'xgb__model__gamma': 0, 'xgb__model__learning_rate': 0.1, 'xgb__model__max_depth': 3, 'xgb__model__min_child_weight': 1.2, 'xgb__model__n_estimators': 1000, 'xgb__model__subsample': 1.0}"
      ],
      "metadata": {
        "id": "DNeru1Wgwn9-",
        "outputId": "78657a14-dec6-46fa-b9ba-e33081959c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DNeru1Wgwn9-",
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing XGBoost Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n",
            "\n",
            "Best XGBoost R²: 0.6434\n",
            "Best XGBoost Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'xgb__feature_selector__feature_tops': (200, 175), 'xgb__feature_selector__rf_max_depth': None, 'xgb__feature_selector__rf_max_features': 'sqrt', 'xgb__feature_selector__rf_min_samples_leaf': 1, 'xgb__feature_selector__rf_n_estimators': 1000, 'xgb__model__colsample_bytree': 1.0, 'xgb__model__gamma': 0, 'xgb__model__learning_rate': 0.1, 'xgb__model__max_depth': 3, 'xgb__model__min_child_weight': 1.2, 'xgb__model__n_estimators': 1000, 'xgb__model__subsample': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "cat_branch = build_cat_branch(random_state=42)\n",
        "\n",
        "cat_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"cat\", cat_branch),\n",
        "])\n",
        "\n",
        "cat_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "\n",
        "    \"cat__feature_selector__feature_tops\": [(200, 179)],\n",
        "    \"cat__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"cat__feature_selector__rf_max_depth\": [None],\n",
        "    \"cat__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"cat__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "\n",
        "\n",
        "    # Number of trees (like n_estimators)\n",
        "    \"cat__model__iterations\": [1000],  # Default 1000\n",
        "\n",
        "    # Learning rate - Lower = need more trees but often better\n",
        "    \"cat__model__learning_rate\": [0.03],  # Default is auto-calculated, 0.03 is common\n",
        "\n",
        "    # Tree depth - Controls model complexity\n",
        "    \"cat__model__depth\": [6],  # Default 6\n",
        "\n",
        "    # L2 regularization on leaf weights\n",
        "    \"cat__model__l2_leaf_reg\": [3],  # Default 3.0\n",
        "\n",
        "    # Minimum number of training samples per leaf\n",
        "    \"cat__model__min_data_in_leaf\": [1],  # Default 1\n",
        "\n",
        "    # Bayesian bootstrap intensity (like subsample)\n",
        "    \"cat__model__bagging_temperature\": [1],  # Default 1.0, higher = more aggressive\n",
        "\n",
        "    # Amount of randomness for scoring splits\n",
        "    \"cat__model__random_strength\": [1],  # Default 1.0\n",
        "}\n",
        "\n",
        "cat_grid = GridSearchCV(\n",
        "    estimator=cat_full_pipeline,\n",
        "    param_grid=cat_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing CatBoost Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cat_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest CatBoost R²: {cat_grid.best_score_:.4f}\")\n",
        "print(f\"Best CatBoost Params: {cat_grid.best_params_}\")\n",
        "\n",
        "# Best CatBoost R²: 0.6490\n",
        "# [\n",
        "#   (50, 45),\n",
        "#   (125, 110),\n",
        "#   (200, 170),\n",
        "#   (275, 230),\n",
        "#   (350, 290),\n",
        "#   (425, 350),\n",
        "#   (500, 410),\n",
        "# ]\n",
        "# (200, 170)\n",
        "\n",
        "# Best CatBoost R²: 0.6552\n",
        "# [\n",
        "#     (200, 171),\n",
        "#     (200, 172),\n",
        "#     (200, 173),\n",
        "#     (200, 174),\n",
        "#     (200, 175),\n",
        "#     (200, 176),\n",
        "#     (200, 177),\n",
        "#     (200, 178),\n",
        "#     (200, 179),\n",
        "# ],\n",
        "# (200, 179),\n",
        "\n",
        "# Best CatBoost R²: 0.6548\n",
        "# [\n",
        "#     (200, 180),\n",
        "#     (200, 181),\n",
        "#     (200, 182),\n",
        "# ],"
      ],
      "metadata": {
        "id": "w0uugEV5GkzJ",
        "outputId": "c2d886db-9633-4a6f-a3f3-d412e4e5295a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "w0uugEV5GkzJ",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing CatBoost Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 180 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 180 features\n",
            "\n",
            "Best CatBoost R²: 0.6548\n",
            "Best CatBoost Params: {'cat__feature_selector__feature_tops': (200, 180), 'cat__feature_selector__rf_max_depth': None, 'cat__feature_selector__rf_max_features': 'sqrt', 'cat__feature_selector__rf_min_samples_leaf': 1, 'cat__feature_selector__rf_n_estimators': 1000, 'cat__model__bagging_temperature': 1, 'cat__model__depth': 6, 'cat__model__iterations': 1000, 'cat__model__l2_leaf_reg': 3, 'cat__model__learning_rate': 0.03, 'cat__model__min_data_in_leaf': 1, 'cat__model__random_strength': 1, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "etr_branch = build_etr_branch(random_state=42)\n",
        "\n",
        "etr_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"etr\", etr_branch),\n",
        "])\n",
        "\n",
        "etr_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "\n",
        "    \"etr__feature_selector__feature_tops\": [\n",
        "        (50, 45),\n",
        "        (125, 110),\n",
        "        (200, 170),\n",
        "        (275, 230),\n",
        "        (350, 290),\n",
        "        (425, 350),\n",
        "        (500, 410),\n",
        "    ],\n",
        "    \"etr__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"etr__feature_selector__rf_max_depth\": [None],\n",
        "    \"etr__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"etr__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "\n",
        "    # ===== ExtraTrees Model Params (DEFAULTS) =====\n",
        "    # MUST-TUNE parameters:\n",
        "\n",
        "    # Number of trees\n",
        "    \"etr__model__n_estimators\": [1000],  # Default 100, but more is often better\n",
        "\n",
        "    # Tree depth - None means unlimited\n",
        "    \"etr__model__max_depth\": [None],  # Default None\n",
        "\n",
        "    # Minimum samples required to be at a leaf node\n",
        "    \"etr__model__min_samples_leaf\": [2],  # Default 1, higher = more regularization\n",
        "\n",
        "    # Number of features to consider when looking for best split\n",
        "    \"etr__model__max_features\": [\"sqrt\"],  # Default \"sqrt\" or 1.0\n",
        "\n",
        "    # Minimum samples required to split an internal node\n",
        "    \"etr__model__min_samples_split\": [2],  # Default 2\n",
        "}\n",
        "\n",
        "etr_grid = GridSearchCV(\n",
        "    estimator=etr_full_pipeline,\n",
        "    param_grid=etr_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing ExtraTrees Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "etr_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest ExtraTrees R²: {etr_grid.best_score_:.4f}\")\n",
        "print(f\"Best ExtraTrees Params: {etr_grid.best_params_}\")\n",
        "\n",
        "# Best ExtraTrees R²: 0.5595\n",
        "# [\n",
        "#     (50, 45),\n",
        "#     (125, 110),\n",
        "#     (200, 170),\n",
        "#     (275, 230),\n",
        "#     (350, 290),\n",
        "#     (425, 350),\n",
        "#     (500, 410),\n",
        "# ],"
      ],
      "metadata": {
        "id": "yowOBBfmMHYd",
        "outputId": "440371ec-0eaa-4cb1-e2e4-059cb8d979de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yowOBBfmMHYd",
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing ExtraTrees Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 125 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 110 features (from 125)\n",
            "[SpearmanRFSelector] FINAL: 110 features\n",
            "\n",
            "Best ExtraTrees R²: 0.5595\n",
            "Best ExtraTrees Params: {'etr__feature_selector__feature_tops': (125, 110), 'etr__feature_selector__rf_max_depth': None, 'etr__feature_selector__rf_max_features': 'sqrt', 'etr__feature_selector__rf_min_samples_leaf': 1, 'etr__feature_selector__rf_n_estimators': 1000, 'etr__model__max_depth': None, 'etr__model__max_features': 'sqrt', 'etr__model__min_samples_leaf': 2, 'etr__model__min_samples_split': 2, 'etr__model__n_estimators': 1000, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpr_kernel = (kernels.ConstantKernel(1.0, (1e-3, 1e3))\n",
        "              * kernels.RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
        "              + kernels.WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-6, 1e1)))\n",
        "\n",
        "# Build GaussianProcess pipeline\n",
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "gpr_branch = build_gpr_branch(random_state=42)\n",
        "\n",
        "gpr_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"gpr\", gpr_branch),\n",
        "])\n",
        "\n",
        "gpr_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "    \"gpr__feature_selector__feature_tops\": [\n",
        "        (200, 175),\n",
        "    ],\n",
        "    \"gpr__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"gpr__feature_selector__rf_max_depth\": [None],\n",
        "    \"gpr__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"gpr__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "\n",
        "    # Kernel - use the custom one defined above\n",
        "    \"gpr__model__kernel\": [gpr_kernel],\n",
        "\n",
        "    # Noise level added to diagonal of kernel matrix (regularization)\n",
        "    \"gpr__model__alpha\": [1e-6],  # Default 1e-10, higher = more regularization\n",
        "\n",
        "    # Whether to normalize target values (often helps)\n",
        "    \"gpr__model__normalize_y\": [True],  # Default False\n",
        "\n",
        "    # Number of restarts for optimizer (more = better but slower)\n",
        "    \"gpr__model__n_restarts_optimizer\": [2],  # Default 0, higher = more robust\n",
        "}\n",
        "\n",
        "gpr_grid = GridSearchCV(\n",
        "    estimator=gpr_full_pipeline,\n",
        "    param_grid=gpr_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=1,\n",
        "    verbose=3,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing GaussianProcess Branch (This will be SLOW!)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "gpr_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest GaussianProcess R²: {gpr_grid.best_score_:.4f}\")\n",
        "print(f\"Best GaussianProcess Params: {gpr_grid.best_params_}\")"
      ],
      "metadata": {
        "id": "eTg7IIBfMOn9",
        "outputId": "6fe424fe-dbfe-4862-d5d2-90b8df127103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eTg7IIBfMOn9",
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing GaussianProcess Branch (This will be SLOW!)\n",
            "============================================================\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 173 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 173 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5] END gpr__feature_selector__feature_tops=(200, 173), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.680 total time=  52.6s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 173 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 173 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5] END gpr__feature_selector__feature_tops=(200, 173), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.631 total time=  39.7s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 173 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 173 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5] END gpr__feature_selector__feature_tops=(200, 173), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.707 total time=  49.6s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 173 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 173 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5] END gpr__feature_selector__feature_tops=(200, 173), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.691 total time=  51.5s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 173 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 173 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5] END gpr__feature_selector__feature_tops=(200, 173), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.685 total time=  50.7s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 174 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 174 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5] END gpr__feature_selector__feature_tops=(200, 174), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.680 total time=  41.5s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 174 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 174 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5] END gpr__feature_selector__feature_tops=(200, 174), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.630 total time=  54.9s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 174 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 174 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5] END gpr__feature_selector__feature_tops=(200, 174), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.708 total time=  51.8s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 174 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 174 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5] END gpr__feature_selector__feature_tops=(200, 174), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.690 total time=  48.0s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 174 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 174 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5] END gpr__feature_selector__feature_tops=(200, 174), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.685 total time=  51.4s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5] END gpr__feature_selector__feature_tops=(200, 175), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.680 total time=  49.7s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5] END gpr__feature_selector__feature_tops=(200, 175), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.630 total time=  45.7s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5] END gpr__feature_selector__feature_tops=(200, 175), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.709 total time=  51.4s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5] END gpr__feature_selector__feature_tops=(200, 175), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.689 total time=  60.0s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5] END gpr__feature_selector__feature_tops=(200, 175), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.684 total time=  56.3s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 176 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 176 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5] END gpr__feature_selector__feature_tops=(200, 176), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.679 total time=  47.6s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 176 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 176 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5] END gpr__feature_selector__feature_tops=(200, 176), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.629 total time= 1.0min\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 176 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 176 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5] END gpr__feature_selector__feature_tops=(200, 176), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.709 total time=  51.3s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 176 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 176 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5] END gpr__feature_selector__feature_tops=(200, 176), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.690 total time=  51.7s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 176 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 176 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5] END gpr__feature_selector__feature_tops=(200, 176), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.685 total time=  50.6s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 177 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 177 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5] END gpr__feature_selector__feature_tops=(200, 177), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.676 total time=  54.6s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 177 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 177 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5] END gpr__feature_selector__feature_tops=(200, 177), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.629 total time=  58.1s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 177 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 177 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5] END gpr__feature_selector__feature_tops=(200, 177), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.708 total time=  46.1s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 177 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 177 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5] END gpr__feature_selector__feature_tops=(200, 177), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.689 total time=  49.6s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 177 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 177 features\n",
            "[CV 5/5] END gpr__feature_selector__feature_tops=(200, 177), gpr__feature_selector__rf_max_depth=None, gpr__feature_selector__rf_max_features=sqrt, gpr__feature_selector__rf_min_samples_leaf=1, gpr__feature_selector__rf_n_estimators=1000, gpr__model__alpha=1e-06, gpr__model__kernel=1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), gpr__model__n_restarts_optimizer=2, gpr__model__normalize_y=True, imputation__knn_imputer__n_neighbors=2, imputation__knn_imputer__weights=distance;, score=0.685 total time=  46.1s\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best GaussianProcess R²: 0.6786\n",
            "Best GaussianProcess Params: {'gpr__feature_selector__feature_tops': (200, 175), 'gpr__feature_selector__rf_max_depth': None, 'gpr__feature_selector__rf_max_features': 'sqrt', 'gpr__feature_selector__rf_min_samples_leaf': 1, 'gpr__feature_selector__rf_n_estimators': 1000, 'gpr__model__alpha': 1e-06, 'gpr__model__kernel': 1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), 'gpr__model__n_restarts_optimizer': 2, 'gpr__model__normalize_y': True, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23aeb7b7",
      "metadata": {
        "id": "23aeb7b7"
      },
      "outputs": [],
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "feature_pipeline = build_feature_selection_pipeline()\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "full_pipeline = Pipeline(\n",
        "    [\n",
        "        (\"imputation\", imputation_pipeline),\n",
        "        (\"feature_selector\", feature_pipeline),\n",
        "        (\"model\", model),\n",
        "    ]\n",
        ")\n",
        "\n",
        "gpr_kernel = (kernels.ConstantKernel(1.0, (1e-3, 1e3))\n",
        "              * kernels.RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
        "              + kernels.WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-6, 1e1)))\n",
        "\n",
        "param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "    # \"imputation__iterative_imputer__estimator\": [NuSVR(kernel='rbf', C=54, gamma=\"scale\")],\n",
        "    # \"imputation__iterative_imputer__max_iter\": [20],\n",
        "    # \"imputation__iterative_imputer__initial_strategy\": [\"median\"],\n",
        "\n",
        "    \"feature_selector__feature_selector__scaler\": [StandardScaler()],\n",
        "    \"feature_selector__feature_selector__spearman_top_k\": [202],\n",
        "    \"feature_selector__feature_selector__rf_top_k\": [173],\n",
        "    \"feature_selector__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"feature_selector__feature_selector__rf_max_depth\": [None],\n",
        "    \"feature_selector__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"feature_selector__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "\n",
        "    # ===== SVR Branch =====\n",
        "    \"model__svr__model__nu\": [0.5],\n",
        "    \"model__svr__model__kernel\": [\"rbf\"],\n",
        "    \"model__svr__model__C\": [55],\n",
        "    \"model__svr__model__gamma\": [\"auto\"],\n",
        "\n",
        "    # ===== HistGradientBoosting Branch =====\n",
        "    \"model__hgb__model__learning_rate\": [0.1],\n",
        "    \"model__hgb__model__max_iter\": [100],\n",
        "    \"model__hgb__model__max_depth\": [None],\n",
        "    \"model__hgb__model__min_samples_leaf\": [20],\n",
        "\n",
        "    # ===== GaussianProcess Branch =====\n",
        "    # \"model__gpr__model__kernel\": [gpr_kernel],\n",
        "    # \"model__gpr__model__alpha\": [1e-6],\n",
        "    # \"model__gpr__model__normalize_y\": [True],\n",
        "    # \"model__gpr__model__n_restarts_optimizer\": [2],\n",
        "\n",
        "    # ===== AdaBoost Branch =====\n",
        "    \"model__abr__model__n_estimators\": [600],\n",
        "    \"model__abr__model__learning_rate\": [0.03],\n",
        "    \"model__abr__model__loss\": [\"square\"],\n",
        "    \"model__abr__model__estimator__max_depth\": [15],\n",
        "    \"model__abr__model__estimator__min_samples_leaf\": [5],\n",
        "\n",
        "    # ===== ExtraTrees Branch =====\n",
        "    \"model__etr__model__n_estimators\": [1000],\n",
        "    \"model__etr__model__max_depth\": [None],\n",
        "    \"model__etr__model__min_samples_leaf\": [2],\n",
        "    \"model__etr__model__max_features\": [\"sqrt\"],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=full_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=1,\n",
        "    verbose=100,\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"Best CV R² (mean across folds): {grid.best_score_:.4f}\")\n",
        "print(\"Best params:\", grid.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca259744",
      "metadata": {
        "id": "ca259744"
      },
      "outputs": [],
      "source": [
        "imputation_pipeline_final = build_imputation_pipeline()\n",
        "feature_pipeline_final = build_feature_selection_pipeline()\n",
        "model_with_gpr = build_model(include_gpr=True)\n",
        "\n",
        "full_pipeline_with_gpr = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline_final),\n",
        "    (\"feature_selector\", feature_pipeline_final),\n",
        "    (\"model\", model_with_gpr),\n",
        "])\n",
        "\n",
        "best_params_with_gpr = grid.best_params_.copy()\n",
        "\n",
        "best_params_with_gpr.update({\n",
        "    \"model__gpr__model__kernel\": gpr_kernel,\n",
        "    \"model__gpr__model__alpha\": 1e-6,\n",
        "    \"model__gpr__model__normalize_y\": True,\n",
        "    \"model__gpr__model__n_restarts_optimizer\": 2,\n",
        "})\n",
        "\n",
        "full_pipeline_with_gpr.set_params(**best_params_with_gpr)\n",
        "full_pipeline_with_gpr.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "y_pred = full_pipeline_with_gpr.predict(x_test_clean)\n",
        "\n",
        "table = pd.DataFrame({'id': np.arange(0, y_pred.shape[0]), 'y': y_pred.flatten()})\n",
        "table.to_csv('submission.csv', index=False)\n",
        "print(\"Submission saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cfdc65f",
      "metadata": {
        "id": "1cfdc65f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}