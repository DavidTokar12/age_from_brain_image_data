{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "e8745ff7",
      "metadata": {
        "id": "e8745ff7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.gaussian_process.kernels as kernels\n",
        "\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import StackingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.svm import NuSVR\n",
        "from joblib import Memory\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import clone\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyoNxh7100Bo",
        "outputId": "b719e5fa-8296-451e-e7b3-3b666ec3bd55"
      },
      "id": "XyoNxh7100Bo",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "78ab9db9",
      "metadata": {
        "id": "78ab9db9"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = Path(\"/content/gdrive/MyDrive/data\")\n",
        "x_train = pd.read_csv(BASE_PATH / 'X_train.csv', skiprows=1, header=None).values[:, 1:]\n",
        "x_test = pd.read_csv(BASE_PATH / 'X_test.csv', skiprows=1, header=None).values[:, 1:]\n",
        "y_train = pd.read_csv(BASE_PATH / 'y_train.csv', skiprows=1, header=None).values[:, 1:].flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "a7687ba6",
      "metadata": {
        "id": "a7687ba6"
      },
      "outputs": [],
      "source": [
        "def remove_outliers(x_train, y_train, x_test, contamination=0.047, variance_threshold=1e-7, random_state=42):\n",
        "    \"\"\"\n",
        "    Remove low-variance features and outliers from training data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    x_train : DataFrame or ndarray\n",
        "    y_train : Series or ndarray\n",
        "    x_test : DataFrame or ndarray\n",
        "    contamination : float, proportion of outliers\n",
        "    variance_threshold : float, threshold for variance filtering\n",
        "    random_state : int\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    x_train_clean, y_train_clean, x_test_clean\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to DataFrame if needed\n",
        "    x_train_df = pd.DataFrame(x_train).copy()\n",
        "    x_test_df = pd.DataFrame(x_test).copy() if x_test is not None else None\n",
        "    y_train_series = pd.Series(np.asarray(y_train).flatten(), index=x_train_df.index)\n",
        "\n",
        "    # ===== Step 1: Remove zero/low-variance features =====\n",
        "    var_selector = VarianceThreshold(threshold=variance_threshold)\n",
        "\n",
        "    # Fit on training data (using median-imputed values to handle NaN)\n",
        "    med = x_train_df.median(axis=0)\n",
        "    x_train_for_var = x_train_df.fillna(med)\n",
        "\n",
        "    var_selector.fit(x_train_for_var)\n",
        "\n",
        "    variance_mask = var_selector.get_support()\n",
        "    n_removed = (~variance_mask).sum()\n",
        "    print(f\"[VarianceThreshold] Removed {n_removed} low-variance features (threshold={variance_threshold})\")\n",
        "\n",
        "    x_train_df = x_train_df.iloc[:, variance_mask]\n",
        "    if x_test_df is not None:\n",
        "        x_test_df = x_test_df.iloc[:, variance_mask]\n",
        "\n",
        "    med = x_train_df.median(axis=0)\n",
        "    xtr_imp = x_train_df.fillna(med)\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    xtr_std = scaler.fit_transform(xtr_imp)\n",
        "\n",
        "    pca = PCA(n_components=2, random_state=random_state)\n",
        "    x_proj = pca.fit_transform(xtr_std)\n",
        "\n",
        "    iso = IsolationForest(contamination=contamination, random_state=random_state)\n",
        "    mask = iso.fit_predict(x_proj) == 1\n",
        "\n",
        "    outlier_pos = np.where(~mask)[0]\n",
        "    n_outliers = outlier_pos.size\n",
        "    print(f\"[OutlierRemoval] Removed {n_outliers} outliers ({n_outliers/len(mask)*100:.2f}%)\")\n",
        "    print(f\"[Outlier positions] {outlier_pos.tolist()}\")\n",
        "\n",
        "    x_train_clean = x_train_df[mask]\n",
        "    y_train_clean = y_train_series[mask]\n",
        "\n",
        "    if isinstance(x_train, np.ndarray):\n",
        "        return x_train_clean.values, y_train_clean.values, x_test_df.values if x_test_df is not None else None\n",
        "    else:\n",
        "        return x_train_clean, y_train_clean, x_test_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledKNNImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    KNN Imputer that internally scales data for distance calculation,\n",
        "    then returns imputed data in the ORIGINAL scale.\n",
        "\n",
        "    This avoids having scaling artifacts in your pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neighbors=7, weights='distance'):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.weights = weights\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_df = pd.DataFrame(X)\n",
        "\n",
        "        self.median_ = X_df.median(axis=0)\n",
        "        X_filled = X_df.fillna(self.median_)\n",
        "\n",
        "        self.scaler_ = StandardScaler()\n",
        "        self.scaler_.fit(X_filled)\n",
        "\n",
        "        X_scaled = self.scaler_.transform(X_filled)\n",
        "        self.imputer_ = KNNImputer(n_neighbors=self.n_neighbors, weights=self.weights)\n",
        "        self.imputer_.fit(X_scaled)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_df = pd.DataFrame(X)\n",
        "\n",
        "        X_filled = X_df.fillna(self.median_)\n",
        "\n",
        "        X_scaled = self.scaler_.transform(X_filled)\n",
        "\n",
        "        X_imputed_scaled = self.imputer_.transform(X_scaled)\n",
        "\n",
        "        X_imputed_original = self.scaler_.inverse_transform(X_imputed_scaled)\n",
        "\n",
        "        return X_imputed_original"
      ],
      "metadata": {
        "id": "jXveDNpyRypS"
      },
      "id": "jXveDNpyRypS",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "381fe6a3",
      "metadata": {
        "id": "381fe6a3"
      },
      "outputs": [],
      "source": [
        "class SpearmanRFSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Combined feature selector:\n",
        "    1. Scale data (using provided scaler)\n",
        "    2. Select top_k features by Spearman correlation\n",
        "    3. Select top_k features by RandomForest importance\n",
        "\n",
        "    Works on already-imputed data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        scaler=None,\n",
        "        feature_tops=(200, 200),  # (spearman_top_k, rf_top_k) as tuple\n",
        "        rf_n_estimators=1000,\n",
        "        rf_max_depth=None,\n",
        "        rf_min_samples_leaf=1,\n",
        "        rf_max_features='sqrt',\n",
        "        random_state=42\n",
        "    ):\n",
        "        self.scaler = scaler\n",
        "        self.feature_tops = feature_tops\n",
        "        self.rf_n_estimators = rf_n_estimators\n",
        "        self.rf_max_depth = rf_max_depth\n",
        "        self.rf_min_samples_leaf = rf_min_samples_leaf\n",
        "        self.rf_max_features = rf_max_features\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if y is None:\n",
        "            raise ValueError(\"SpearmanRFSelector requires y\")\n",
        "\n",
        "        spearman_top_k, rf_top_k = self.feature_tops\n",
        "\n",
        "        X_df = pd.DataFrame(X).copy() if not isinstance(X, pd.DataFrame) else X.copy()\n",
        "        y_series = pd.Series(np.asarray(y).flatten(), index=X_df.index)\n",
        "\n",
        "        self.n_features_in_ = X_df.shape[1]\n",
        "\n",
        "        if self.scaler is not None:\n",
        "            self.scaler_ = clone(self.scaler)\n",
        "            X_scaled = self.scaler_.fit_transform(X_df)\n",
        "            X_scaled_df = pd.DataFrame(X_scaled, columns=X_df.columns, index=X_df.index)\n",
        "        else:\n",
        "            X_scaled_df = X_df\n",
        "\n",
        "        spearman_corr = X_scaled_df.corrwith(y_series, method='spearman').abs()\n",
        "\n",
        "        n_keep_spearman = min(spearman_top_k, len(spearman_corr))\n",
        "        spearman_features = spearman_corr.nlargest(n_keep_spearman).index\n",
        "\n",
        "        print(f\"[SpearmanRFSelector] Spearman: Selected {len(spearman_features)} features (from {self.n_features_in_})\")\n",
        "\n",
        "        X_spearman = X_scaled_df.loc[:, spearman_features]\n",
        "\n",
        "        self.rf_ = RandomForestRegressor(\n",
        "            n_estimators=self.rf_n_estimators,\n",
        "            max_depth=self.rf_max_depth,\n",
        "            min_samples_leaf=self.rf_min_samples_leaf,\n",
        "            max_features=self.rf_max_features,\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        self.rf_.fit(X_spearman, y_series)\n",
        "\n",
        "        importances = pd.Series(self.rf_.feature_importances_, index=X_spearman.columns)\n",
        "        importances = importances.sort_values(ascending=False)\n",
        "\n",
        "        n_keep_rf = min(rf_top_k, len(importances)) if rf_top_k else len(importances)\n",
        "        rf_features = importances.head(n_keep_rf).index\n",
        "\n",
        "        print(f\"[SpearmanRFSelector] RF: Selected {len(rf_features)} features (from {len(spearman_features)})\")\n",
        "\n",
        "        self.selected_features_ = [X_df.columns.get_loc(col) for col in rf_features]\n",
        "        self.selected_feature_names_ = rf_features.tolist()\n",
        "\n",
        "        print(f\"[SpearmanRFSelector] FINAL: {len(self.selected_features_)} features\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
        "        # Return selected features\n",
        "        return X_df.iloc[:, self.selected_features_].values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_imputation_pipeline(random_state=42):\n",
        "    pipeline = Pipeline(\n",
        "        [\n",
        "              (\"knn_imputer\", ScaledKNNImputer()),\n",
        "            # (\"iterative_imputer\", IterativeImputer()),\n",
        "        ]\n",
        "    )\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "ivhM2nOC4W7A"
      },
      "id": "ivhM2nOC4W7A",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "668ce9c7",
      "metadata": {
        "id": "668ce9c7"
      },
      "outputs": [],
      "source": [
        "def build_feature_selection_pipeline(random_state=42):\n",
        "\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            random_state=random_state\n",
        "        ))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "98862227",
      "metadata": {
        "id": "98862227"
      },
      "outputs": [],
      "source": [
        "def build_svr_branch(random_state=42):\n",
        "    \"\"\"SVR branch with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", NuSVR())\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_hgb_branch(random_state=42):\n",
        "    \"\"\"HistGradientBoosting with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", HistGradientBoostingRegressor(random_state=random_state))\n",
        "    ])\n",
        "\n",
        "def build_abr_branch(random_state=42):\n",
        "    \"\"\"AdaBoost with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", AdaBoostRegressor(\n",
        "            estimator=DecisionTreeRegressor(random_state=random_state),\n",
        "            random_state=random_state\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_etr_branch(random_state=42):\n",
        "    \"\"\"ExtraTrees with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", ExtraTreesRegressor(random_state=random_state, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "def build_gpr_branch(random_state=42):\n",
        "    \"\"\"GaussianProcess with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", RobustScaler()),\n",
        "        (\"model\", GaussianProcessRegressor(random_state=random_state))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "f7484d3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7484d3a",
        "outputId": "ba9cc5da-d32f-4a4c-c085-6f934c726092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VarianceThreshold] Removed 4 low-variance features (threshold=1e-07)\n",
            "[OutlierRemoval] Removed 57 outliers (4.70%)\n",
            "[Outlier positions] [3, 10, 27, 60, 114, 156, 167, 203, 207, 213, 232, 329, 346, 380, 382, 423, 441, 442, 446, 459, 537, 575, 579, 611, 627, 674, 676, 681, 690, 694, 697, 733, 740, 763, 769, 805, 899, 906, 915, 933, 935, 951, 983, 1015, 1032, 1070, 1071, 1087, 1090, 1097, 1103, 1127, 1136, 1137, 1144, 1152, 1196]\n"
          ]
        }
      ],
      "source": [
        "x_train_clean, y_train_clean, x_test_clean = remove_outliers(\n",
        "    x_train, y_train, x_test, contamination=0.047, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "svr_branch = build_svr_branch(random_state=42)\n",
        "\n",
        "svr_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"svr\", svr_branch),\n",
        "])\n",
        "\n",
        "svr_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "    \"svr__feature_selector__feature_tops\": [(200, 175)],\n",
        "    \"svr__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"svr__feature_selector__rf_max_depth\": [None],\n",
        "    \"svr__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"svr__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "    \"svr__model__nu\": [0.5],\n",
        "    \"svr__model__kernel\": [\"rbf\"],\n",
        "    \"svr__model__C\": [60],\n",
        "    \"svr__model__gamma\": [\"auto\"],\n",
        "}\n",
        "\n",
        "svr_grid = GridSearchCV(\n",
        "    estimator=svr_full_pipeline,\n",
        "    param_grid=svr_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing SVR Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "svr_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest SVR R²: {svr_grid.best_score_:.4f}\")\n",
        "print(f\"Best SVR Params: {svr_grid.best_params_}\")\n",
        "\n",
        "#  {\n",
        "#     \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "#     \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "#     \"svr__feature_selector__feature_tops\": [\n",
        "#         (50, 45),\n",
        "#         (75, 65),\n",
        "#         (100, 90),\n",
        "#         (125, 110),\n",
        "#         (150, 130),\n",
        "#         (175, 150),\n",
        "#         (200, 170),\n",
        "#         (225, 190),\n",
        "#         (250, 210),\n",
        "#         (275, 230),\n",
        "#         (300, 250),\n",
        "#         (350, 290),\n",
        "#         (400, 330),\n",
        "#         (450, 370),\n",
        "#         (500, 410),\n",
        "#     ],\n",
        "#     \"svr__feature_selector__rf_n_estimators\": [1000],\n",
        "#     \"svr__feature_selector__rf_max_depth\": [None],\n",
        "#     \"svr__feature_selector__rf_min_samples_leaf\": [1],\n",
        "#     \"svr__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "#     \"svr__model__nu\": [0.5],\n",
        "#     \"svr__model__kernel\": [\"rbf\"],\n",
        "#     \"svr__model__C\": [50, 55, 60],\n",
        "#     \"svr__model__gamma\": [\"auto\"],\n",
        "# }\n",
        "# Best SVR R²: 0.6731\n",
        "# Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 170), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n",
        "\n",
        "\n",
        "# \"svr__feature_selector__feature_tops\": [\n",
        "#       (170, 145),\n",
        "#       (175, 150),\n",
        "#       (180, 155),\n",
        "#       (185, 160),\n",
        "#       (190, 165),\n",
        "#       (195, 168),\n",
        "#       (198, 168),\n",
        "#       (200, 170),\n",
        "#       (202, 172),\n",
        "#       (205, 175),\n",
        "#       (210, 180),\n",
        "#       (215, 185),\n",
        "#       (220, 188),\n",
        "#       (225, 190),\n",
        "#       (230, 195),\n",
        "# ]\n",
        "# Best SVR R²: 0.6731\n",
        "# Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 170), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n",
        "\n",
        "# \"svr__feature_selector__feature_tops\": [\n",
        "#       (200, 161),\n",
        "#       (200, 162),\n",
        "#       (200, 163),\n",
        "#       (200, 164),\n",
        "#       (200, 165),\n",
        "#       (200, 166),\n",
        "#       (200, 167),\n",
        "#       (200, 168),\n",
        "#       (200, 169),\n",
        "#       (200, 170),\n",
        "#       (200, 171),\n",
        "#       (200, 172),\n",
        "#       (200, 173),\n",
        "#       (200, 174),\n",
        "#       (200, 175),\n",
        "#       (200, 176),\n",
        "#       (200, 177),\n",
        "#       (200, 178),\n",
        "#       (200, 179),\n",
        "#     ],\n",
        "# Best SVR R²: 0.6756\n",
        "# Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 175), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n",
        "\n",
        "# \"svr__model__nu\": [0.4, 0.5, 0.6],\n",
        "# \"svr__model__C\": [60, 65, 70],\n",
        "# Best SVR R²: 0.6756\n",
        "# Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 175), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n"
      ],
      "metadata": {
        "id": "hk6xBz_mIoJz",
        "outputId": "3628c747-dac6-4bfa-e2d2-ccea7844f8a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hk6xBz_mIoJz",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing SVR Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n",
            "\n",
            "Best SVR R²: 0.6756\n",
            "Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 175), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "hgb_branch = build_hgb_branch(random_state=42)\n",
        "\n",
        "hgb_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"hgb\", hgb_branch),\n",
        "])\n",
        "\n",
        "hgb_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "\n",
        "    \"hgb__feature_selector__feature_tops\":[(200, 167)],\n",
        "    \"hgb__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"hgb__feature_selector__rf_max_depth\": [None],\n",
        "    \"hgb__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"hgb__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "\n",
        "    \"hgb__model__learning_rate\": [0.05, 0.1, 0.15],\n",
        "    \"hgb__model__max_iter\": [100, 150, 200],\n",
        "    \"hgb__model__max_depth\": [None],\n",
        "    \"hgb__model__min_samples_leaf\": [20],\n",
        "}\n",
        "\n",
        "hgb_grid = GridSearchCV(\n",
        "    estimator=hgb_full_pipeline,\n",
        "    param_grid=hgb_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing HGB Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "hgb_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest HGB R²: {hgb_grid.best_score_:.4f}\")\n",
        "print(f\"Best HGB Params: {hgb_grid.best_params_}\")\n",
        "\n",
        "\n",
        "# [\n",
        "#  (50, 45),\n",
        "#  (125, 110),\n",
        "#  (200, 170),\n",
        "#  (275, 230),\n",
        "#  (350, 290),\n",
        "#  (425, 350),\n",
        "#  (500, 410),\n",
        "# ],\n",
        "# Best HGB R²: 0.6367\n",
        "# Best HGB Params: {'hgb__feature_selector__feature_tops': (200, 170), 'hgb__feature_selector__rf_max_depth': None, 'hgb__feature_selector__rf_max_features': 'sqrt', 'hgb__feature_selector__rf_min_samples_leaf': 1, 'hgb__feature_selector__rf_n_estimators': 1000, 'hgb__model__learning_rate': 0.1, 'hgb__model__max_depth': None, 'hgb__model__max_iter': 100, 'hgb__model__min_samples_leaf': 20, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}\n",
        "\n",
        "\n",
        "# [\n",
        "#       (200, 161),\n",
        "#       (200, 162),\n",
        "#       (200, 163),\n",
        "#       (200, 164),\n",
        "#       (200, 165),\n",
        "#       (200, 166),\n",
        "#       (200, 167),\n",
        "#       (200, 168),\n",
        "#       (200, 169),\n",
        "#       (200, 170),\n",
        "#       (200, 171),\n",
        "#       (200, 172),\n",
        "#       (200, 173),\n",
        "#       (200, 174),\n",
        "#       (200, 175),\n",
        "#       (200, 176),\n",
        "#       (200, 177),\n",
        "#       (200, 178),\n",
        "#       (200, 179),\n",
        "# ]\n",
        "# Best HGB R²: 0.6424\n",
        "# Best HGB Params: {'hgb__feature_selector__feature_tops': (200, 167), 'hgb__feature_selector__rf_max_depth': None, 'hgb__feature_selector__rf_max_features': 'sqrt', 'hgb__feature_selector__rf_min_samples_leaf': 1, 'hgb__feature_selector__rf_n_estimators': 1000, 'hgb__model__learning_rate': 0.1, 'hgb__model__max_depth': None, 'hgb__model__max_iter': 100, 'hgb__model__min_samples_leaf': 20, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}"
      ],
      "metadata": {
        "id": "RgdzIIBEqMzz",
        "outputId": "2034043d-998c-466d-d4ec-da41ad657b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RgdzIIBEqMzz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing HGB Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23aeb7b7",
      "metadata": {
        "id": "23aeb7b7"
      },
      "outputs": [],
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "feature_pipeline = build_feature_selection_pipeline()\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "full_pipeline = Pipeline(\n",
        "    [\n",
        "        (\"imputation\", imputation_pipeline),\n",
        "        (\"feature_selector\", feature_pipeline),\n",
        "        (\"model\", model),\n",
        "    ]\n",
        ")\n",
        "\n",
        "gpr_kernel = (kernels.ConstantKernel(1.0, (1e-3, 1e3))\n",
        "              * kernels.RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
        "              + kernels.WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-6, 1e1)))\n",
        "\n",
        "param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "    # \"imputation__iterative_imputer__estimator\": [NuSVR(kernel='rbf', C=54, gamma=\"scale\")],\n",
        "    # \"imputation__iterative_imputer__max_iter\": [20],\n",
        "    # \"imputation__iterative_imputer__initial_strategy\": [\"median\"],\n",
        "\n",
        "    \"feature_selector__feature_selector__scaler\": [StandardScaler()],\n",
        "    \"feature_selector__feature_selector__spearman_top_k\": [202],\n",
        "    \"feature_selector__feature_selector__rf_top_k\": [173],\n",
        "    \"feature_selector__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"feature_selector__feature_selector__rf_max_depth\": [None],\n",
        "    \"feature_selector__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"feature_selector__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "\n",
        "    # ===== SVR Branch =====\n",
        "    \"model__svr__model__nu\": [0.5],\n",
        "    \"model__svr__model__kernel\": [\"rbf\"],\n",
        "    \"model__svr__model__C\": [55],\n",
        "    \"model__svr__model__gamma\": [\"auto\"],\n",
        "\n",
        "    # ===== HistGradientBoosting Branch =====\n",
        "    \"model__hgb__model__learning_rate\": [0.1],\n",
        "    \"model__hgb__model__max_iter\": [100],\n",
        "    \"model__hgb__model__max_depth\": [None],\n",
        "    \"model__hgb__model__min_samples_leaf\": [20],\n",
        "\n",
        "    # ===== GaussianProcess Branch =====\n",
        "    # \"model__gpr__model__kernel\": [gpr_kernel],\n",
        "    # \"model__gpr__model__alpha\": [1e-6],\n",
        "    # \"model__gpr__model__normalize_y\": [True],\n",
        "    # \"model__gpr__model__n_restarts_optimizer\": [2],\n",
        "\n",
        "    # ===== AdaBoost Branch =====\n",
        "    \"model__abr__model__n_estimators\": [600],\n",
        "    \"model__abr__model__learning_rate\": [0.03],\n",
        "    \"model__abr__model__loss\": [\"square\"],\n",
        "    \"model__abr__model__estimator__max_depth\": [15],\n",
        "    \"model__abr__model__estimator__min_samples_leaf\": [5],\n",
        "\n",
        "    # ===== ExtraTrees Branch =====\n",
        "    \"model__etr__model__n_estimators\": [1000],\n",
        "    \"model__etr__model__max_depth\": [None],\n",
        "    \"model__etr__model__min_samples_leaf\": [2],\n",
        "    \"model__etr__model__max_features\": [\"sqrt\"],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=full_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=1,\n",
        "    verbose=100,\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"Best CV R² (mean across folds): {grid.best_score_:.4f}\")\n",
        "print(\"Best params:\", grid.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca259744",
      "metadata": {
        "id": "ca259744"
      },
      "outputs": [],
      "source": [
        "imputation_pipeline_final = build_imputation_pipeline()\n",
        "feature_pipeline_final = build_feature_selection_pipeline()\n",
        "model_with_gpr = build_model(include_gpr=True)\n",
        "\n",
        "full_pipeline_with_gpr = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline_final),\n",
        "    (\"feature_selector\", feature_pipeline_final),\n",
        "    (\"model\", model_with_gpr),\n",
        "])\n",
        "\n",
        "best_params_with_gpr = grid.best_params_.copy()\n",
        "\n",
        "best_params_with_gpr.update({\n",
        "    \"model__gpr__model__kernel\": gpr_kernel,\n",
        "    \"model__gpr__model__alpha\": 1e-6,\n",
        "    \"model__gpr__model__normalize_y\": True,\n",
        "    \"model__gpr__model__n_restarts_optimizer\": 2,\n",
        "})\n",
        "\n",
        "full_pipeline_with_gpr.set_params(**best_params_with_gpr)\n",
        "full_pipeline_with_gpr.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "y_pred = full_pipeline_with_gpr.predict(x_test_clean)\n",
        "\n",
        "table = pd.DataFrame({'id': np.arange(0, y_pred.shape[0]), 'y': y_pred.flatten()})\n",
        "table.to_csv('submission.csv', index=False)\n",
        "print(\"Submission saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cfdc65f",
      "metadata": {
        "id": "1cfdc65f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}