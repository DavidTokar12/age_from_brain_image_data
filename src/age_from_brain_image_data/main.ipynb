{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 159,
      "id": "e8745ff7",
      "metadata": {
        "id": "e8745ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0524214-a786-4229-cd91-ef85dbaac114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.gaussian_process.kernels as kernels\n",
        "\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import StackingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.svm import NuSVR\n",
        "from joblib import Memory\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import clone\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "eQvw6nGFv2Ye"
      },
      "id": "eQvw6nGFv2Ye",
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyoNxh7100Bo",
        "outputId": "fbf6f427-6919-4265-9730-09909f9541fb"
      },
      "id": "XyoNxh7100Bo",
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "id": "78ab9db9",
      "metadata": {
        "id": "78ab9db9"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = Path(\"/content/gdrive/MyDrive/data\")\n",
        "x_train = pd.read_csv(BASE_PATH / 'X_train.csv', skiprows=1, header=None).values[:, 1:]\n",
        "x_test = pd.read_csv(BASE_PATH / 'X_test.csv', skiprows=1, header=None).values[:, 1:]\n",
        "y_train = pd.read_csv(BASE_PATH / 'y_train.csv', skiprows=1, header=None).values[:, 1:].flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "id": "a7687ba6",
      "metadata": {
        "id": "a7687ba6"
      },
      "outputs": [],
      "source": [
        "def remove_outliers(x_train, y_train, x_test, contamination=0.047, variance_threshold=1e-7, random_state=42):\n",
        "    \"\"\"\n",
        "    Remove low-variance features and outliers from training data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    x_train : DataFrame or ndarray\n",
        "    y_train : Series or ndarray\n",
        "    x_test : DataFrame or ndarray\n",
        "    contamination : float, proportion of outliers\n",
        "    variance_threshold : float, threshold for variance filtering\n",
        "    random_state : int\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    x_train_clean, y_train_clean, x_test_clean\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to DataFrame if needed\n",
        "    x_train_df = pd.DataFrame(x_train).copy()\n",
        "    x_test_df = pd.DataFrame(x_test).copy() if x_test is not None else None\n",
        "    y_train_series = pd.Series(np.asarray(y_train).flatten(), index=x_train_df.index)\n",
        "\n",
        "    # ===== Step 1: Remove zero/low-variance features =====\n",
        "    var_selector = VarianceThreshold(threshold=variance_threshold)\n",
        "\n",
        "    # Fit on training data (using median-imputed values to handle NaN)\n",
        "    med = x_train_df.median(axis=0)\n",
        "    x_train_for_var = x_train_df.fillna(med)\n",
        "\n",
        "    var_selector.fit(x_train_for_var)\n",
        "\n",
        "    variance_mask = var_selector.get_support()\n",
        "    n_removed = (~variance_mask).sum()\n",
        "    print(f\"[VarianceThreshold] Removed {n_removed} low-variance features (threshold={variance_threshold})\")\n",
        "\n",
        "    x_train_df = x_train_df.iloc[:, variance_mask]\n",
        "    if x_test_df is not None:\n",
        "        x_test_df = x_test_df.iloc[:, variance_mask]\n",
        "\n",
        "    med = x_train_df.median(axis=0)\n",
        "    xtr_imp = x_train_df.fillna(med)\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    xtr_std = scaler.fit_transform(xtr_imp)\n",
        "\n",
        "    pca = PCA(n_components=2, random_state=random_state)\n",
        "    x_proj = pca.fit_transform(xtr_std)\n",
        "\n",
        "    iso = IsolationForest(contamination=contamination, random_state=random_state)\n",
        "    mask = iso.fit_predict(x_proj) == 1\n",
        "\n",
        "    outlier_pos = np.where(~mask)[0]\n",
        "    n_outliers = outlier_pos.size\n",
        "    print(f\"[OutlierRemoval] Removed {n_outliers} outliers ({n_outliers/len(mask)*100:.2f}%)\")\n",
        "    print(f\"[Outlier positions] {outlier_pos.tolist()}\")\n",
        "\n",
        "    x_train_clean = x_train_df[mask]\n",
        "    y_train_clean = y_train_series[mask]\n",
        "\n",
        "    if isinstance(x_train, np.ndarray):\n",
        "        return x_train_clean.values, y_train_clean.values, x_test_df.values if x_test_df is not None else None\n",
        "    else:\n",
        "        return x_train_clean, y_train_clean, x_test_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledKNNImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    KNN Imputer that internally scales data for distance calculation,\n",
        "    then returns imputed data in the ORIGINAL scale.\n",
        "\n",
        "    This avoids having scaling artifacts in your pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neighbors=7, weights='distance'):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.weights = weights\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_df = pd.DataFrame(X)\n",
        "\n",
        "        self.median_ = X_df.median(axis=0)\n",
        "        X_filled = X_df.fillna(self.median_)\n",
        "\n",
        "        self.scaler_ = StandardScaler()\n",
        "        self.scaler_.fit(X_filled)\n",
        "\n",
        "        X_scaled = self.scaler_.transform(X_filled)\n",
        "        self.imputer_ = KNNImputer(n_neighbors=self.n_neighbors, weights=self.weights)\n",
        "        self.imputer_.fit(X_scaled)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_df = pd.DataFrame(X)\n",
        "\n",
        "        X_filled = X_df.fillna(self.median_)\n",
        "\n",
        "        X_scaled = self.scaler_.transform(X_filled)\n",
        "\n",
        "        X_imputed_scaled = self.imputer_.transform(X_scaled)\n",
        "\n",
        "        X_imputed_original = self.scaler_.inverse_transform(X_imputed_scaled)\n",
        "\n",
        "        return X_imputed_original"
      ],
      "metadata": {
        "id": "jXveDNpyRypS"
      },
      "id": "jXveDNpyRypS",
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "381fe6a3",
      "metadata": {
        "id": "381fe6a3"
      },
      "outputs": [],
      "source": [
        "class SpearmanRFSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Combined feature selector:\n",
        "    1. Scale data (using provided scaler)\n",
        "    2. Select top_k features by Spearman correlation\n",
        "    3. Select top_k features by RandomForest importance\n",
        "\n",
        "    Works on already-imputed data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        scaler=None,\n",
        "        feature_tops=(200, 200),  # (spearman_top_k, rf_top_k) as tuple\n",
        "        rf_n_estimators=1000,\n",
        "        rf_max_depth=None,\n",
        "        rf_min_samples_leaf=1,\n",
        "        rf_max_features='sqrt',\n",
        "        random_state=42\n",
        "    ):\n",
        "        self.scaler = scaler\n",
        "        self.feature_tops = feature_tops\n",
        "        self.rf_n_estimators = rf_n_estimators\n",
        "        self.rf_max_depth = rf_max_depth\n",
        "        self.rf_min_samples_leaf = rf_min_samples_leaf\n",
        "        self.rf_max_features = rf_max_features\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if y is None:\n",
        "            raise ValueError(\"SpearmanRFSelector requires y\")\n",
        "\n",
        "        spearman_top_k, rf_top_k = self.feature_tops\n",
        "\n",
        "        X_df = pd.DataFrame(X).copy() if not isinstance(X, pd.DataFrame) else X.copy()\n",
        "        y_series = pd.Series(np.asarray(y).flatten(), index=X_df.index)\n",
        "\n",
        "        self.n_features_in_ = X_df.shape[1]\n",
        "\n",
        "        if self.scaler is not None:\n",
        "            self.scaler_ = clone(self.scaler)\n",
        "            X_scaled = self.scaler_.fit_transform(X_df)\n",
        "            X_scaled_df = pd.DataFrame(X_scaled, columns=X_df.columns, index=X_df.index)\n",
        "        else:\n",
        "            X_scaled_df = X_df\n",
        "\n",
        "        spearman_corr = X_scaled_df.corrwith(y_series, method='spearman').abs()\n",
        "\n",
        "        n_keep_spearman = min(spearman_top_k, len(spearman_corr))\n",
        "        spearman_features = spearman_corr.nlargest(n_keep_spearman).index\n",
        "\n",
        "        print(f\"[SpearmanRFSelector] Spearman: Selected {len(spearman_features)} features (from {self.n_features_in_})\")\n",
        "\n",
        "        X_spearman = X_scaled_df.loc[:, spearman_features]\n",
        "\n",
        "        self.rf_ = RandomForestRegressor(\n",
        "            n_estimators=self.rf_n_estimators,\n",
        "            max_depth=self.rf_max_depth,\n",
        "            min_samples_leaf=self.rf_min_samples_leaf,\n",
        "            max_features=self.rf_max_features,\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        self.rf_.fit(X_spearman, y_series)\n",
        "\n",
        "        importances = pd.Series(self.rf_.feature_importances_, index=X_spearman.columns)\n",
        "        importances = importances.sort_values(ascending=False)\n",
        "\n",
        "        n_keep_rf = min(rf_top_k, len(importances)) if rf_top_k else len(importances)\n",
        "        rf_features = importances.head(n_keep_rf).index\n",
        "\n",
        "        print(f\"[SpearmanRFSelector] RF: Selected {len(rf_features)} features (from {len(spearman_features)})\")\n",
        "\n",
        "        self.selected_features_ = [X_df.columns.get_loc(col) for col in rf_features]\n",
        "        self.selected_feature_names_ = rf_features.tolist()\n",
        "\n",
        "        print(f\"[SpearmanRFSelector] FINAL: {len(self.selected_features_)} features\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
        "        # Return selected features\n",
        "        return X_df.iloc[:, self.selected_features_].values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_imputation_pipeline(random_state=42):\n",
        "    pipeline = Pipeline(\n",
        "        [\n",
        "              (\"knn_imputer\", ScaledKNNImputer()),\n",
        "            # (\"iterative_imputer\", IterativeImputer()),\n",
        "        ]\n",
        "    )\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "ivhM2nOC4W7A"
      },
      "id": "ivhM2nOC4W7A",
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "668ce9c7",
      "metadata": {
        "id": "668ce9c7"
      },
      "outputs": [],
      "source": [
        "def build_feature_selection_pipeline(random_state=42):\n",
        "\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            random_state=random_state\n",
        "        ))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "98862227",
      "metadata": {
        "id": "98862227"
      },
      "outputs": [],
      "source": [
        "def build_svr_branch(random_state=42):\n",
        "    \"\"\"SVR branch with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", NuSVR())\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_hgb_branch(random_state=42):\n",
        "    \"\"\"HistGradientBoosting with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", HistGradientBoostingRegressor(random_state=random_state))\n",
        "    ])\n",
        "\n",
        "def build_xgb_branch(random_state=42):\n",
        "    \"\"\"XGBoost\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", RobustScaler()),\n",
        "        (\"model\", XGBRegressor(\n",
        "            random_state=random_state,\n",
        "            n_jobs=-1,\n",
        "            verbosity=0  # Suppress output\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_cat_branch(random_state=42):\n",
        "    \"\"\"CatBoost\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", RobustScaler()),\n",
        "        (\"model\", CatBoostRegressor(\n",
        "            random_state=random_state,\n",
        "            verbose=False,\n",
        "            thread_count=-1\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_etr_branch(random_state=42):\n",
        "    \"\"\"ExtraTrees with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", ExtraTreesRegressor(random_state=random_state, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "def build_gpr_branch(random_state=42):\n",
        "    \"\"\"GaussianProcess with feature selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", RobustScaler()),\n",
        "        (\"model\", GaussianProcessRegressor(random_state=random_state))\n",
        "    ])\n",
        "\n",
        "# TODO try with ensemble\n",
        "def build_ridge_branch(random_state=42):\n",
        "    \"\"\"Ridge - Simple linear model\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", Ridge())\n",
        "    ])\n",
        "\n",
        "# TODO try with ensemble\n",
        "def build_pca_ridge_branch(random_state=42):\n",
        "    \"\"\"Ridge with PCA instead of SpearmanRF selection\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", PCA(n_components=100, random_state=random_state)),  # Different approach!\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", Ridge())\n",
        "    ])\n",
        "\n",
        "\n",
        "# TODO try with ensemble\n",
        "def build_mlp_branch(random_state=42):\n",
        "    \"\"\"MLP - Multi-layer Perceptron (Neural Network)\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"feature_selector\", SpearmanRFSelector(\n",
        "            scaler=StandardScaler(),\n",
        "            random_state=random_state\n",
        "        )),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", MLPRegressor(random_state=random_state, max_iter=1000, early_stopping=True))\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "f7484d3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7484d3a",
        "outputId": "216073d7-a537-4ede-9923-89cd950725f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VarianceThreshold] Removed 4 low-variance features (threshold=1e-07)\n",
            "[OutlierRemoval] Removed 57 outliers (4.70%)\n",
            "[Outlier positions] [3, 10, 27, 60, 114, 156, 167, 203, 207, 213, 232, 329, 346, 380, 382, 423, 441, 442, 446, 459, 537, 575, 579, 611, 627, 674, 676, 681, 690, 694, 697, 733, 740, 763, 769, 805, 899, 906, 915, 933, 935, 951, 983, 1015, 1032, 1070, 1071, 1087, 1090, 1097, 1103, 1127, 1136, 1137, 1144, 1152, 1196]\n"
          ]
        }
      ],
      "source": [
        "x_train_clean, y_train_clean, x_test_clean = remove_outliers(\n",
        "    x_train, y_train, x_test, contamination=0.047, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "svr_branch = build_svr_branch(random_state=42)\n",
        "\n",
        "svr_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"svr\", svr_branch),\n",
        "])\n",
        "\n",
        "svr_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "    \"svr__feature_selector__feature_tops\": [(200, 175)],\n",
        "    \"svr__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"svr__feature_selector__rf_max_depth\": [None],\n",
        "    \"svr__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"svr__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "    \"svr__model__nu\": [0.5],\n",
        "    \"svr__model__kernel\": [\"rbf\"],\n",
        "    \"svr__model__C\": [60],\n",
        "    \"svr__model__gamma\": [\"auto\"],\n",
        "}\n",
        "\n",
        "svr_grid = GridSearchCV(\n",
        "    estimator=svr_full_pipeline,\n",
        "    param_grid=svr_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing SVR Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "svr_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest SVR R²: {svr_grid.best_score_:.4f}\")\n",
        "print(f\"Best SVR Params: {svr_grid.best_params_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk6xBz_mIoJz",
        "outputId": "00e3d09a-c741-468a-c8c1-17248394d3f8"
      },
      "id": "hk6xBz_mIoJz",
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing SVR Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n",
            "\n",
            "Best SVR R²: 0.6756\n",
            "Best SVR Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'svr__feature_selector__feature_tops': (200, 175), 'svr__feature_selector__rf_max_depth': None, 'svr__feature_selector__rf_max_features': 'sqrt', 'svr__feature_selector__rf_min_samples_leaf': 1, 'svr__feature_selector__rf_n_estimators': 1000, 'svr__model__C': 60, 'svr__model__gamma': 'auto', 'svr__model__kernel': 'rbf', 'svr__model__nu': 0.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "hgb_branch = build_hgb_branch(random_state=42)\n",
        "\n",
        "hgb_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"hgb\", hgb_branch),\n",
        "])\n",
        "\n",
        "hgb_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "\n",
        "    \"hgb__feature_selector__feature_tops\":[(200, 167)],\n",
        "    \"hgb__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"hgb__feature_selector__rf_max_depth\": [None],\n",
        "    \"hgb__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"hgb__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "\n",
        "    \"hgb__model__learning_rate\": [0.1],\n",
        "    \"hgb__model__max_iter\": [200],\n",
        "    \"hgb__model__max_depth\": [None],\n",
        "    \"hgb__model__min_samples_leaf\": [20],\n",
        "}\n",
        "\n",
        "hgb_grid = GridSearchCV(\n",
        "    estimator=hgb_full_pipeline,\n",
        "    param_grid=hgb_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing HGB Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "hgb_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest HGB R²: {hgb_grid.best_score_:.4f}\")\n",
        "print(f\"Best HGB Params: {hgb_grid.best_params_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgdzIIBEqMzz",
        "outputId": "7669ca39-e526-4a52-db57-5939c0306f6c"
      },
      "id": "RgdzIIBEqMzz",
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing HGB Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 167 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 167 features\n",
            "\n",
            "Best HGB R²: 0.6449\n",
            "Best HGB Params: {'hgb__feature_selector__feature_tops': (200, 167), 'hgb__feature_selector__rf_max_depth': None, 'hgb__feature_selector__rf_max_features': 'sqrt', 'hgb__feature_selector__rf_min_samples_leaf': 1, 'hgb__feature_selector__rf_n_estimators': 1000, 'hgb__model__learning_rate': 0.1, 'hgb__model__max_depth': None, 'hgb__model__max_iter': 200, 'hgb__model__min_samples_leaf': 20, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "xgb_branch = build_xgb_branch(random_state=42)\n",
        "\n",
        "xgb_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"xgb\", xgb_branch),\n",
        "])\n",
        "\n",
        "xgb_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "    \"xgb__feature_selector__feature_tops\": [(200, 175)],\n",
        "    \"xgb__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"xgb__feature_selector__rf_max_depth\": [None],\n",
        "    \"xgb__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"xgb__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "    \"xgb__model__n_estimators\": [1000],\n",
        "    \"xgb__model__learning_rate\": [0.1],\n",
        "    \"xgb__model__max_depth\": [3],\n",
        "    \"xgb__model__min_child_weight\": [1.2],\n",
        "    \"xgb__model__subsample\": [1.0],\n",
        "    \"xgb__model__colsample_bytree\": [1.0],\n",
        "    \"xgb__model__gamma\": [0],\n",
        "}\n",
        "\n",
        "xgb_grid = GridSearchCV(\n",
        "    estimator=xgb_full_pipeline,\n",
        "    param_grid=xgb_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing XGBoost Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "xgb_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest XGBoost R²: {xgb_grid.best_score_:.4f}\")\n",
        "print(f\"Best XGBoost Params: {xgb_grid.best_params_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNeru1Wgwn9-",
        "outputId": "1912dccd-b55c-4b6f-fd7e-85aae2f800b3"
      },
      "id": "DNeru1Wgwn9-",
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing XGBoost Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n",
            "\n",
            "Best XGBoost R²: 0.6434\n",
            "Best XGBoost Params: {'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance', 'xgb__feature_selector__feature_tops': (200, 175), 'xgb__feature_selector__rf_max_depth': None, 'xgb__feature_selector__rf_max_features': 'sqrt', 'xgb__feature_selector__rf_min_samples_leaf': 1, 'xgb__feature_selector__rf_n_estimators': 1000, 'xgb__model__colsample_bytree': 1.0, 'xgb__model__gamma': 0, 'xgb__model__learning_rate': 0.1, 'xgb__model__max_depth': 3, 'xgb__model__min_child_weight': 1.2, 'xgb__model__n_estimators': 1000, 'xgb__model__subsample': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "cat_branch = build_cat_branch(random_state=42)\n",
        "\n",
        "cat_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"cat\", cat_branch),\n",
        "])\n",
        "\n",
        "cat_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "\n",
        "    \"cat__feature_selector__feature_tops\": [(200, 179)],\n",
        "    \"cat__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"cat__feature_selector__rf_max_depth\": [None],\n",
        "    \"cat__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"cat__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "\n",
        "\n",
        "    # Number of trees (like n_estimators)\n",
        "    \"cat__model__iterations\": [1000],  # Default 1000\n",
        "\n",
        "    # Learning rate - Lower = need more trees but often better\n",
        "    \"cat__model__learning_rate\": [0.03],  # Default is auto-calculated, 0.03 is common\n",
        "\n",
        "    # Tree depth - Controls model complexity\n",
        "    \"cat__model__depth\": [6],  # Default 6\n",
        "\n",
        "    # L2 regularization on leaf weights\n",
        "    \"cat__model__l2_leaf_reg\": [3],  # Default 3.0\n",
        "\n",
        "    # Minimum number of training samples per leaf\n",
        "    \"cat__model__min_data_in_leaf\": [1],  # Default 1\n",
        "\n",
        "    # Bayesian bootstrap intensity (like subsample)\n",
        "    \"cat__model__bagging_temperature\": [1],  # Default 1.0, higher = more aggressive\n",
        "\n",
        "    # Amount of randomness for scoring splits\n",
        "    \"cat__model__random_strength\": [1],  # Default 1.0\n",
        "}\n",
        "\n",
        "cat_grid = GridSearchCV(\n",
        "    estimator=cat_full_pipeline,\n",
        "    param_grid=cat_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing CatBoost Branch\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cat_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest CatBoost R²: {cat_grid.best_score_:.4f}\")\n",
        "print(f\"Best CatBoost Params: {cat_grid.best_params_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0uugEV5GkzJ",
        "outputId": "2f879fa4-5763-432d-f6f5-80634b149d86"
      },
      "id": "w0uugEV5GkzJ",
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing CatBoost Branch\n",
            "============================================================\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 179 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 179 features\n",
            "\n",
            "Best CatBoost R²: 0.6552\n",
            "Best CatBoost Params: {'cat__feature_selector__feature_tops': (200, 179), 'cat__feature_selector__rf_max_depth': None, 'cat__feature_selector__rf_max_features': 'sqrt', 'cat__feature_selector__rf_min_samples_leaf': 1, 'cat__feature_selector__rf_n_estimators': 1000, 'cat__model__bagging_temperature': 1, 'cat__model__depth': 6, 'cat__model__iterations': 1000, 'cat__model__l2_leaf_reg': 3, 'cat__model__learning_rate': 0.03, 'cat__model__min_data_in_leaf': 1, 'cat__model__random_strength': 1, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpr_kernel = (kernels.ConstantKernel(1.0, (1e-3, 1e3))\n",
        "              * kernels.RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
        "              + kernels.WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-6, 1e1)))\n",
        "\n",
        "# Build GaussianProcess pipeline\n",
        "imputation_pipeline = build_imputation_pipeline()\n",
        "\n",
        "gpr_branch = build_gpr_branch(random_state=42)\n",
        "\n",
        "gpr_full_pipeline = Pipeline([\n",
        "    (\"imputation\", imputation_pipeline),\n",
        "    (\"gpr\", gpr_branch),\n",
        "])\n",
        "\n",
        "gpr_param_grid = {\n",
        "    \"imputation__knn_imputer__n_neighbors\": [2],\n",
        "    \"imputation__knn_imputer__weights\": [\"distance\"],\n",
        "    \"gpr__feature_selector__feature_tops\": [\n",
        "        (200, 175),\n",
        "    ],\n",
        "    \"gpr__feature_selector__rf_n_estimators\": [1000],\n",
        "    \"gpr__feature_selector__rf_max_depth\": [None],\n",
        "    \"gpr__feature_selector__rf_min_samples_leaf\": [1],\n",
        "    \"gpr__feature_selector__rf_max_features\": [\"sqrt\"],\n",
        "\n",
        "    # Kernel - use the custom one defined above\n",
        "    \"gpr__model__kernel\": [gpr_kernel],\n",
        "\n",
        "    # Noise level added to diagonal of kernel matrix (regularization)\n",
        "    \"gpr__model__alpha\": [1e-6],  # Default 1e-10, higher = more regularization\n",
        "\n",
        "    # Whether to normalize target values (often helps)\n",
        "    \"gpr__model__normalize_y\": [True],  # Default False\n",
        "\n",
        "    # Number of restarts for optimizer (more = better but slower)\n",
        "    \"gpr__model__n_restarts_optimizer\": [2],  # Default 0, higher = more robust\n",
        "}\n",
        "\n",
        "gpr_grid = GridSearchCV(\n",
        "    estimator=gpr_full_pipeline,\n",
        "    param_grid=gpr_param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    n_jobs=-1,\n",
        "    verbose=3,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Optimizing GaussianProcess Branch (This will be SLOW!)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "gpr_grid.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "print(f\"\\nBest GaussianProcess R²: {gpr_grid.best_score_:.4f}\")\n",
        "print(f\"Best GaussianProcess Params: {gpr_grid.best_params_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTg7IIBfMOn9",
        "outputId": "a3bab9c2-f5e9-4d53-cfc6-53acbe95acbb"
      },
      "id": "eTg7IIBfMOn9",
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Optimizing GaussianProcess Branch (This will be SLOW!)\n",
            "============================================================\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[SpearmanRFSelector] Spearman: Selected 200 features (from 828)\n",
            "[SpearmanRFSelector] RF: Selected 175 features (from 200)\n",
            "[SpearmanRFSelector] FINAL: 175 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best GaussianProcess R²: 0.6786\n",
            "Best GaussianProcess Params: {'gpr__feature_selector__feature_tops': (200, 175), 'gpr__feature_selector__rf_max_depth': None, 'gpr__feature_selector__rf_max_features': 'sqrt', 'gpr__feature_selector__rf_min_samples_leaf': 1, 'gpr__feature_selector__rf_n_estimators': 1000, 'gpr__model__alpha': 1e-06, 'gpr__model__kernel': 1**2 * RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=0.001), 'gpr__model__n_restarts_optimizer': 2, 'gpr__model__normalize_y': True, 'imputation__knn_imputer__n_neighbors': 2, 'imputation__knn_imputer__weights': 'distance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVR\n",
        "print(\"\\n[1/5] SVR - Generating CV predictions...\")\n",
        "svr_pipeline = svr_grid.best_estimator_\n",
        "y_svr_cv_pred = cross_val_predict(svr_pipeline, x_train_clean, y_train_clean, cv=5, n_jobs=-1)\n",
        "print(f\"SVR CV predictions shape: {y_svr_cv_pred.shape}\")\n",
        "\n",
        "# HGB\n",
        "print(\"\\n[2/5] HGB - Generating CV predictions...\")\n",
        "hgb_pipeline = hgb_grid.best_estimator_\n",
        "y_hgb_cv_pred = cross_val_predict(hgb_pipeline, x_train_clean, y_train_clean, cv=5, n_jobs=-1)\n",
        "print(f\"HGB CV predictions shape: {y_hgb_cv_pred.shape}\")\n",
        "\n",
        "# XGB\n",
        "print(\"\\n[3/5] XGB - Generating CV predictions...\")\n",
        "xgb_pipeline = xgb_grid.best_estimator_\n",
        "y_xgb_cv_pred = cross_val_predict(xgb_pipeline, x_train_clean, y_train_clean, cv=5, n_jobs=-1)\n",
        "print(f\"XGB CV predictions shape: {y_xgb_cv_pred.shape}\")\n",
        "\n",
        "# CAT\n",
        "print(\"\\n[4/5] CAT - Generating CV predictions...\")\n",
        "cat_pipeline = cat_grid.best_estimator_\n",
        "y_cat_cv_pred = cross_val_predict(cat_pipeline, x_train_clean, y_train_clean, cv=5, n_jobs=-1)\n",
        "print(f\"CAT CV predictions shape: {y_cat_cv_pred.shape}\")\n",
        "\n",
        "# GPR\n",
        "print(\"\\n[5/5] GPR - Generating CV predictions...\")\n",
        "gpr_pipeline = gpr_grid.best_estimator_\n",
        "y_gpr_cv_pred = cross_val_predict(gpr_pipeline, x_train_clean, y_train_clean, cv=5, n_jobs=-1)\n",
        "print(f\"GPR CV predictions shape: {y_gpr_cv_pred.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlYMq3N5oVMq",
        "outputId": "6cc65925-b2a9-496a-b0a1-fdfb1b14a970"
      },
      "id": "UlYMq3N5oVMq",
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/5] SVR - Generating CV predictions...\n",
            "SVR CV predictions shape: (1155,)\n",
            "\n",
            "[2/5] HGB - Generating CV predictions...\n",
            "HGB CV predictions shape: (1155,)\n",
            "\n",
            "[3/5] XGB - Generating CV predictions...\n",
            "XGB CV predictions shape: (1155,)\n",
            "\n",
            "[4/5] CAT - Generating CV predictions...\n",
            "CAT CV predictions shape: (1155,)\n",
            "\n",
            "[5/5] GPR - Generating CV predictions...\n",
            "GPR CV predictions shape: (1155,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_dict = {\n",
        "    'SVR': y_svr_cv_pred,\n",
        "    'HGB': y_hgb_cv_pred,\n",
        "    'XGB': y_xgb_cv_pred,\n",
        "    'CAT': y_cat_cv_pred,\n",
        "    'GPR': y_gpr_cv_pred,\n",
        "}\n",
        "\n",
        "model_names = list(models_dict.keys())\n",
        "n_models = len(model_names)\n",
        "\n",
        "# ========================================================================\n",
        "# SECTION 1: INDIVIDUAL MODEL PERFORMANCE\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 1: Individual Model Performance\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "performance_data = []\n",
        "for name, preds in models_dict.items():\n",
        "    r2 = r2_score(y_train_clean, preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y_train_clean, preds))\n",
        "    mae = mean_absolute_error(y_train_clean, preds)\n",
        "    performance_data.append({\n",
        "        'Model': name,\n",
        "        'R²': r2,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae\n",
        "    })\n",
        "\n",
        "performance_df = pd.DataFrame(performance_data).sort_values('R²', ascending=False)\n",
        "print(\"\\n\" + performance_df.to_string(index=False))\n",
        "\n",
        "best_model = performance_df.iloc[0]['Model']\n",
        "best_r2 = performance_df.iloc[0]['R²']\n",
        "print(f\"\\nBest Individual Model: {best_model} (R² = {best_r2:.4f})\")\n",
        "\n",
        "# ========================================================================\n",
        "# SECTION 2: PREDICTION CORRELATION ANALYSIS\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 2: Prediction Correlation Matrix (Diversity Analysis)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "corr_matrix = np.zeros((n_models, n_models))\n",
        "for i, name_i in enumerate(model_names):\n",
        "    for j, name_j in enumerate(model_names):\n",
        "        corr, _ = pearsonr(models_dict[name_i], models_dict[name_j])\n",
        "        corr_matrix[i, j] = corr\n",
        "\n",
        "corr_df = pd.DataFrame(corr_matrix, index=model_names, columns=model_names)\n",
        "print(\"\\n\" + corr_df.to_string())\n",
        "\n",
        "# Find highly correlated pairs (> 0.90)\n",
        "print(\"\\nHigh Correlation Pairs (r > 0.90):\")\n",
        "high_corr_pairs = []\n",
        "for i in range(n_models):\n",
        "    for j in range(i+1, n_models):\n",
        "        if corr_matrix[i, j] > 0.90:\n",
        "            print(f\"  {model_names[i]} ↔ {model_names[j]}: r = {corr_matrix[i, j]:.3f} (Very similar!)\")\n",
        "            high_corr_pairs.append((model_names[i], model_names[j], corr_matrix[i, j]))\n",
        "\n",
        "if not high_corr_pairs:\n",
        "    print(\"  None found - Good diversity!\")\n",
        "\n",
        "# Find low correlation pairs (< 0.75)\n",
        "print(\"\\nLow Correlation Pairs (r < 0.75):\")\n",
        "low_corr_pairs = []\n",
        "for i in range(n_models):\n",
        "    for j in range(i+1, n_models):\n",
        "        if corr_matrix[i, j] < 0.75:\n",
        "            print(f\"  {model_names[i]} ↔ {model_names[j]}: r = {corr_matrix[i, j]:.3f} ✅ (Good diversity!)\")\n",
        "            low_corr_pairs.append((model_names[i], model_names[j], corr_matrix[i, j]))\n",
        "\n",
        "# ========================================================================\n",
        "# SECTION 3: ERROR ANALYSIS\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 3: Error Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate errors for each model\n",
        "errors_dict = {}\n",
        "abs_errors_dict = {}\n",
        "for name, preds in models_dict.items():\n",
        "    errors_dict[name] = y_train_clean - preds\n",
        "    abs_errors_dict[name] = np.abs(y_train_clean - preds)\n",
        "\n",
        "# Error correlation matrix (do models make errors on same samples?)\n",
        "error_corr_matrix = np.zeros((n_models, n_models))\n",
        "for i, name_i in enumerate(model_names):\n",
        "    for j, name_j in enumerate(model_names):\n",
        "        corr, _ = pearsonr(abs_errors_dict[name_i], abs_errors_dict[name_j])\n",
        "        error_corr_matrix[i, j] = corr\n",
        "\n",
        "error_corr_df = pd.DataFrame(error_corr_matrix, index=model_names, columns=model_names)\n",
        "print(\"\\nError Correlation Matrix (Do models fail on same samples?):\")\n",
        "print(error_corr_df.to_string())\n",
        "\n",
        "print(\"\\nHigh Error Correlation Pairs (r > 0.85):\")\n",
        "for i in range(n_models):\n",
        "    for j in range(i+1, n_models):\n",
        "        if error_corr_matrix[i, j] > 0.85:\n",
        "            print(f\"  {model_names[i]} ↔ {model_names[j]}: r = {error_corr_matrix[i, j]:.3f} (Fail on same samples)\")\n",
        "\n",
        "# ========================================================================\n",
        "# SECTION 4: BEST MODEL PER SAMPLE\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 4: Which Model Predicts Each Sample Best?\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# For each sample, find which model has lowest absolute error\n",
        "all_abs_errors = np.array([abs_errors_dict[name] for name in model_names])\n",
        "best_model_per_sample = np.argmin(all_abs_errors, axis=0)\n",
        "\n",
        "# Count how many samples each model predicts best\n",
        "best_counts = {name: 0 for name in model_names}\n",
        "for idx in best_model_per_sample:\n",
        "    best_counts[model_names[idx]] += 1\n",
        "\n",
        "print(\"\\nSamples Best Predicted by Each Model:\")\n",
        "total_samples = len(y_train_clean)\n",
        "for name in model_names:\n",
        "    count = best_counts[name]\n",
        "    percentage = (count / total_samples) * 100\n",
        "    print(f\"  {name}: {count:4d} samples ({percentage:5.2f}%)\")\n",
        "\n",
        "# ========================================================================\n",
        "# SECTION 5: COVERAGE ANALYSIS\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 5: Coverage Analysis (Oracle Ensemble)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Oracle: For each sample, use the best prediction available\n",
        "oracle_predictions = np.zeros_like(y_train_clean)\n",
        "for i in range(len(y_train_clean)):\n",
        "    best_model_idx = best_model_per_sample[i]\n",
        "    oracle_predictions[i] = models_dict[model_names[best_model_idx]][i]\n",
        "\n",
        "oracle_r2 = r2_score(y_train_clean, oracle_predictions)\n",
        "oracle_rmse = np.sqrt(mean_squared_error(y_train_clean, oracle_predictions))\n",
        "oracle_mae = mean_absolute_error(y_train_clean, oracle_predictions)\n",
        "\n",
        "print(f\"\\nOracle Ensemble Performance (Perfect Model Selection per Sample):\")\n",
        "print(f\"  R²:   {oracle_r2:.4f}\")\n",
        "print(f\"  RMSE: {oracle_rmse:.4f}\")\n",
        "print(f\"  MAE:  {oracle_mae:.4f}\")\n",
        "\n",
        "print(f\"\\nImprovement vs Best Individual Model:\")\n",
        "print(f\"  Best Individual: {best_model} (R² = {best_r2:.4f})\")\n",
        "print(f\"  Oracle:          R² = {oracle_r2:.4f}\")\n",
        "print(f\"  Potential Gain:  ΔR² = {oracle_r2 - best_r2:.4f}\")\n",
        "\n",
        "threshold = performance_df.iloc[0]['MAE']\n",
        "print(f\"\\nCoverage Analysis (threshold = {threshold:.2f} MAE):\")\n",
        "\n",
        "n_models_good_per_sample = np.zeros(total_samples)\n",
        "for name, abs_errors in abs_errors_dict.items():\n",
        "    n_models_good_per_sample += (abs_errors <= threshold).astype(int)\n",
        "\n",
        "coverage_stats = {\n",
        "    'Covered by 0 models': np.sum(n_models_good_per_sample == 0),\n",
        "    'Covered by 1 model': np.sum(n_models_good_per_sample == 1),\n",
        "    'Covered by 2 models': np.sum(n_models_good_per_sample == 2),\n",
        "    'Covered by 3 models': np.sum(n_models_good_per_sample == 3),\n",
        "    'Covered by 4 models': np.sum(n_models_good_per_sample == 4),\n",
        "    'Covered by 5 models': np.sum(n_models_good_per_sample == 5),\n",
        "}\n",
        "\n",
        "print(\"\\nSamples Covered by N Models (within threshold):\")\n",
        "for key, count in coverage_stats.items():\n",
        "    percentage = (count / total_samples) * 100\n",
        "    print(f\"  {key}: {count:4d} samples ({percentage:5.2f}%)\")\n",
        "\n",
        "coverage_any = np.sum(n_models_good_per_sample >= 1)\n",
        "coverage_any_pct = (coverage_any / total_samples) * 100\n",
        "print(f\"\\nTotal samples covered by at least 1 model: {coverage_any} ({coverage_any_pct:.2f}%)\")\n",
        "\n",
        "uncovered = np.sum(n_models_good_per_sample == 0)\n",
        "uncovered_pct = (uncovered / total_samples) * 100\n",
        "print(f\"Samples where all models struggle: {uncovered} ({uncovered_pct:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2NqncHqpkaa",
        "outputId": "ccfef217-53da-4116-b211-66e65e01477d"
      },
      "id": "L2NqncHqpkaa",
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SECTION 1: Individual Model Performance\n",
            "================================================================================\n",
            "\n",
            "Model       R²     RMSE      MAE\n",
            "  GPR 0.678723 5.282552 3.992635\n",
            "  SVR 0.675945 5.305339 3.988044\n",
            "  CAT 0.656357 5.463334 4.256988\n",
            "  HGB 0.645021 5.552710 4.335817\n",
            "  XGB 0.644223 5.558949 4.321796\n",
            "\n",
            "Best Individual Model: GPR (R² = 0.6787)\n",
            "\n",
            "================================================================================\n",
            "SECTION 2: Prediction Correlation Matrix (Diversity Analysis)\n",
            "================================================================================\n",
            "\n",
            "          SVR       HGB       XGB       CAT       GPR\n",
            "SVR  1.000000  0.896356  0.911060  0.923528  0.978499\n",
            "HGB  0.896356  1.000000  0.955211  0.961466  0.922791\n",
            "XGB  0.911060  0.955211  1.000000  0.966840  0.931238\n",
            "CAT  0.923528  0.961466  0.966840  1.000000  0.945391\n",
            "GPR  0.978499  0.922791  0.931238  0.945391  1.000000\n",
            "\n",
            "High Correlation Pairs (r > 0.90):\n",
            "  SVR ↔ XGB: r = 0.911 (Very similar!)\n",
            "  SVR ↔ CAT: r = 0.924 (Very similar!)\n",
            "  SVR ↔ GPR: r = 0.978 (Very similar!)\n",
            "  HGB ↔ XGB: r = 0.955 (Very similar!)\n",
            "  HGB ↔ CAT: r = 0.961 (Very similar!)\n",
            "  HGB ↔ GPR: r = 0.923 (Very similar!)\n",
            "  XGB ↔ CAT: r = 0.967 (Very similar!)\n",
            "  XGB ↔ GPR: r = 0.931 (Very similar!)\n",
            "  CAT ↔ GPR: r = 0.945 (Very similar!)\n",
            "\n",
            "Low Correlation Pairs (r < 0.75):\n",
            "\n",
            "================================================================================\n",
            "SECTION 3: Error Analysis\n",
            "================================================================================\n",
            "\n",
            "Error Correlation Matrix (Do models fail on same samples?):\n",
            "          SVR       HGB       XGB       CAT       GPR\n",
            "SVR  1.000000  0.666655  0.710301  0.739769  0.914621\n",
            "HGB  0.666655  1.000000  0.840300  0.866499  0.736185\n",
            "XGB  0.710301  0.840300  1.000000  0.877066  0.757770\n",
            "CAT  0.739769  0.866499  0.877066  1.000000  0.797573\n",
            "GPR  0.914621  0.736185  0.757770  0.797573  1.000000\n",
            "\n",
            "High Error Correlation Pairs (r > 0.85):\n",
            "  SVR ↔ GPR: r = 0.915 (Fail on same samples)\n",
            "  HGB ↔ CAT: r = 0.866 (Fail on same samples)\n",
            "  XGB ↔ CAT: r = 0.877 (Fail on same samples)\n",
            "\n",
            "================================================================================\n",
            "SECTION 4: Which Model Predicts Each Sample Best?\n",
            "================================================================================\n",
            "\n",
            "Samples Best Predicted by Each Model:\n",
            "  SVR:  318 samples (27.53%)\n",
            "  HGB:  225 samples (19.48%)\n",
            "  XGB:  211 samples (18.27%)\n",
            "  CAT:  184 samples (15.93%)\n",
            "  GPR:  217 samples (18.79%)\n",
            "\n",
            "================================================================================\n",
            "SECTION 5: Coverage Analysis (Oracle Ensemble)\n",
            "================================================================================\n",
            "\n",
            "Oracle Ensemble Performance (Perfect Model Selection per Sample):\n",
            "  R²:   0.8283\n",
            "  RMSE: 3.8618\n",
            "  MAE:  2.5399\n",
            "\n",
            "Improvement vs Best Individual Model:\n",
            "  Best Individual: GPR (R² = 0.6787)\n",
            "  Oracle:          R² = 0.8283\n",
            "  Potential Gain:  ΔR² = 0.1496\n",
            "\n",
            "Coverage Analysis (threshold = 3.99 MAE):\n",
            "\n",
            "Samples Covered by N Models (within threshold):\n",
            "  Covered by 0 models:  275 samples (23.81%)\n",
            "  Covered by 1 model:   86 samples ( 7.45%)\n",
            "  Covered by 2 models:  104 samples ( 9.00%)\n",
            "  Covered by 3 models:  120 samples (10.39%)\n",
            "  Covered by 4 models:  150 samples (12.99%)\n",
            "  Covered by 5 models:  420 samples (36.36%)\n",
            "\n",
            "Total samples covered by at least 1 model: 880 (76.19%)\n",
            "Samples where all models struggle: 275 (23.81%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "id": "23aeb7b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23aeb7b7",
        "outputId": "45add323-4b05-4e8d-f82a-ff0985f61368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Stacking Ensemble:\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed: 19.5min remaining: 29.2min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stacking (2 models) CV R²: 0.6940 ± 0.0268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 19.9min finished\n"
          ]
        }
      ],
      "source": [
        "def build_stacking_model():\n",
        "    stacking_model = StackingRegressor(\n",
        "        estimators=[\n",
        "            (\"gpr\", gpr_grid.best_estimator_),\n",
        "            (\"xgb\", xgb_grid.best_estimator_),\n",
        "\n",
        "            (\"svr\", svr_grid.best_estimator_),\n",
        "            (\"cat\", cat_grid.best_estimator_),\n",
        "            (\"hgb\", hgb_grid.best_estimator_)\n",
        "        ],\n",
        "        final_estimator=LinearRegression(),\n",
        "\n",
        "    # TODO try, can select the useless models\n",
        "    # final_estimator=ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=10000),\n",
        "\n",
        "    # TODO try out possibly\n",
        "    # final_estimator=XGBRegressor(\n",
        "    #     n_estimators=100,\n",
        "    #     max_depth=3,  # Keep shallow!\n",
        "    #     learning_rate=0.1,\n",
        "    #     random_state=42\n",
        "    # ),\n",
        "    # TODO try out possibly\n",
        "    #final_estimator=MLPRegressor(\n",
        "    #     hidden_layer_sizes=(10, 5),  # Keep small!\n",
        "    #     alpha=1.0,  # Strong regularization\n",
        "    #     max_iter=1000,\n",
        "    #     early_stopping=True,\n",
        "    #     random_state=42\n",
        "    # ),\n",
        "\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    return stacking_model\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Stacking Ensemble:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "stacking = build_stacking_model()\n",
        "\n",
        "scores = cross_val_score(\n",
        "    stacking,\n",
        "    x_train_clean,\n",
        "    y_train_clean,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "print(f\"\\nStacking CV R²: {scores.mean():.4f} ± {scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stacking.fit(x_train_clean, y_train_clean)\n",
        "\n",
        "y_train_pred = stacking.predict(x_train_clean)\n",
        "train_r2 = r2_score(y_train_clean, y_train_pred)\n",
        "print(f\"Training R²: {train_r2:.4f}\")\n",
        "\n",
        "y_pred = stacking.predict(x_test_clean)\n",
        "\n",
        "table = pd.DataFrame({'id': np.arange(0, y_pred.shape[0]), 'y': y_pred.flatten()})\n",
        "table.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0MAqwNmt5HH",
        "outputId": "3ef4acda-fec5-42bc-9e39-269bf4d7164d"
      },
      "id": "-0MAqwNmt5HH",
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training R²: 0.9918\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}